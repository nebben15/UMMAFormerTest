{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchviz import make_dot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import eval\n",
    "import test_stuff_functions as tsf\n",
    "\n",
    "from libs.core import load_config\n",
    "from libs.datasets import make_dataset, make_data_loader\n",
    "from libs.modeling import make_meta_arch\n",
    "from libs.utils import fix_random_seed\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'audio_feat_folder': './data/Psynd/feats/byola',\n",
      "             'audio_file_ext': None,\n",
      "             'audio_input_dim': 2048,\n",
      "             'crop_ratio': [0.9, 1.0],\n",
      "             'default_fps': None,\n",
      "             'downsample_rate': 1,\n",
      "             'feat_stride': 1,\n",
      "             'file_ext': '.npy',\n",
      "             'file_prefix': 'none',\n",
      "             'force_upsampling': True,\n",
      "             'input_dim': 0,\n",
      "             'json_file': './data/Psynd/annotations/metadatabyola.json',\n",
      "             'max_seq_len': 768,\n",
      "             'num_classes': 1,\n",
      "             'num_frames': 1,\n",
      "             'trunc_thresh': 0.5},\n",
      " 'dataset_name': 'psynd',\n",
      " 'devices': [0],\n",
      " 'init_rand_seed': 1234567891,\n",
      " 'loader': {'batch_size': 8, 'num_workers': 4},\n",
      " 'model': {'audio_input_dim': 2048,\n",
      "           'backbone_arch': (2, 2, 5),\n",
      "           'backbone_type': 'convHRLRFullResSelfAttTransformerRevised',\n",
      "           'embd_dim': 256,\n",
      "           'embd_kernel_size': 3,\n",
      "           'embd_with_ln': True,\n",
      "           'fpn_dim': 256,\n",
      "           'fpn_start_level': 0,\n",
      "           'fpn_type': 'fpn',\n",
      "           'fpn_with_ln': True,\n",
      "           'head_dim': 256,\n",
      "           'head_kernel_size': 3,\n",
      "           'head_num_layers': 3,\n",
      "           'head_with_ln': True,\n",
      "           'input_dim': 0,\n",
      "           'max_buffer_len_factor': 1.0,\n",
      "           'max_seq_len': 768,\n",
      "           'n_head': 4,\n",
      "           'n_mha_win_size': [7, 7, 7, 7, 7, -1],\n",
      "           'num_classes': 1,\n",
      "           'regression_range': [(0, 4),\n",
      "                                (4, 8),\n",
      "                                (8, 16),\n",
      "                                (16, 32),\n",
      "                                (32, 64),\n",
      "                                (64, 10000)],\n",
      "           'scale_factor': 2,\n",
      "           'test_cfg': {'duration_thresh': 0.001,\n",
      "                        'ext_score_file': None,\n",
      "                        'iou_threshold': 0.1,\n",
      "                        'max_seg_num': 100,\n",
      "                        'min_score': 0.001,\n",
      "                        'multiclass_nms': False,\n",
      "                        'nms_method': 'soft',\n",
      "                        'nms_sigma': 0.75,\n",
      "                        'pre_nms_thresh': 0.001,\n",
      "                        'pre_nms_topk': 2000,\n",
      "                        'voting_thresh': 0.9},\n",
      "           'train_cfg': {'center_sample': 'radius',\n",
      "                         'center_sample_radius': 1.5,\n",
      "                         'clip_grad_l2norm': 1.0,\n",
      "                         'cls_prior_prob': 0.01,\n",
      "                         'dropout': 0.0,\n",
      "                         'droppath': 0.1,\n",
      "                         'head_empty_cls': [],\n",
      "                         'init_loss_norm': 200,\n",
      "                         'label_smoothing': 0.1,\n",
      "                         'loss_weight': 2.0},\n",
      "           'use_abs_pe': True,\n",
      "           'use_rel_pe': False},\n",
      " 'model_name': 'AVLocPointTransformerRecoveryNoNorm',\n",
      " 'opt': {'epochs': 10,\n",
      "         'learning_rate': 0.001,\n",
      "         'momentum': 0.9,\n",
      "         'schedule_gamma': 0.1,\n",
      "         'schedule_steps': [],\n",
      "         'schedule_type': 'cosine',\n",
      "         'type': 'AdamW',\n",
      "         'warmup': True,\n",
      "         'warmup_epochs': 5,\n",
      "         'weight_decay': 0.05},\n",
      " 'output_folder': './paper_results/Revised',\n",
      " 'test_cfg': {'duration_thresh': 0.001,\n",
      "              'ext_score_file': None,\n",
      "              'iou_threshold': 0.1,\n",
      "              'max_seg_num': 100,\n",
      "              'min_score': 0.001,\n",
      "              'multiclass_nms': False,\n",
      "              'nms_method': 'soft',\n",
      "              'nms_sigma': 0.75,\n",
      "              'pre_nms_thresh': 0.001,\n",
      "              'pre_nms_topk': 2000,\n",
      "              'voting_thresh': 0.9},\n",
      " 'test_split': ['test'],\n",
      " 'train_cfg': {'center_sample': 'radius',\n",
      "               'center_sample_radius': 1.5,\n",
      "               'clip_grad_l2norm': 1.0,\n",
      "               'cls_prior_prob': 0.01,\n",
      "               'dropout': 0.0,\n",
      "               'droppath': 0.1,\n",
      "               'head_empty_cls': [],\n",
      "               'init_loss_norm': 200,\n",
      "               'label_smoothing': 0.1,\n",
      "               'loss_weight': 2.0},\n",
      " 'train_split': ['train'],\n",
      " 'val_split': ['dev']}\n",
      "['test'] subset has 79 videos\n",
      "=> loading checkpoint 'Pretrained/psynd_byola/epoch_015.pth.tar'\n",
      "Loading from EMA model ...\n",
      "\n",
      "Start testing model AVLocPointTransformerRecoveryNoNorm ...\n",
      "Test: [00010/00079]\tTime 0.17 (0.17)\n",
      "Test: [00020/00079]\tTime 0.14 (0.15)\n",
      "Test: [00030/00079]\tTime 0.13 (0.15)\n",
      "Test: [00040/00079]\tTime 0.13 (0.14)\n",
      "Test: [00050/00079]\tTime 0.14 (0.14)\n",
      "Test: [00060/00079]\tTime 0.13 (0.14)\n",
      "Test: [00070/00079]\tTime 0.13 (0.14)\n",
      "saving detection results...\n",
      "evaluion detection results...\n",
      "{'Fake': 0}\n",
      "[INIT] Loaded annotations from test subset.\n",
      "\tNumber of ground truth instances: 79\n",
      "\tNumber of predictions: 7900\n",
      "\tFixed threshold for tiou score: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "[RESULTS] Performance on  detection task.\n",
      "Average-mAP: 0.9697027938387699\n",
      "Detection: average-mAP 96.970 mAP@0.50 100.000 mAP@0.55 100.000 mAP@0.60 100.000 mAP@0.65 100.000 mAP@0.70 100.000 mAP@0.75 98.574 mAP@0.80 98.574 mAP@0.85 97.322 mAP@0.90 95.363 mAP@0.95 79.870\n",
      "evaluion proposal results...\n",
      "{'Fake': 0}\n",
      "[INIT] Loaded annotations from test subset.\n",
      "\tNumber of ground truth instances: 79\n",
      "\tNumber of proposals: 7900\n",
      "\tFixed threshold for tiou score: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "[RESULTS] Performance on  proposal task.\n",
      "\tArea Under the AR vs AN curve: 96.6177215189873%\n",
      "All done! Total time: 21.01 sec\n"
     ]
    }
   ],
   "source": [
    "eval.run(config='configs/UMMAFormer/psynd_byola.yaml', ckpt='Pretrained/psynd_byola/epoch_015.pth.tar', epoch=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Results:\n",
      "   average-mAP  mAP@0.50  mAP@0.55  mAP@0.60  mAP@0.65  mAP@0.70  mAP@0.75  \\\n",
      "0        96.97     100.0     100.0     100.0     100.0     100.0    98.574   \n",
      "\n",
      "   mAP@0.80  mAP@0.85  mAP@0.90  mAP@0.95  \n",
      "0    98.574    97.322    95.363     79.87  \n",
      "\n",
      "Proposal Results:\n",
      "    AR@10   AR@20   AR@50  AR@100\n",
      "0  97.595  97.595  97.595  97.595\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/ben/Thesis/UMMAFormerTest/Pretrained/psynd_byola/test_results.txt'\n",
    "detection_df, proposal_df = tsf.format_results_as_matrix(file_path)\n",
    "\n",
    "# Display the matrices\n",
    "print(\"Detection Results:\")\n",
    "print(detection_df)\n",
    "\n",
    "print(\"\\nProposal Results:\")\n",
    "print(proposal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to load\n",
    "config='configs/UMMAFormer/psynd_byola.yaml'\n",
    "ckpt='Pretrained/psynd_byola/epoch_015.pth.tar'\n",
    "epoch=15\n",
    "_ = fix_random_seed(0, include_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "if os.path.isfile(config):\n",
    "    cfg = load_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test'] subset has 79 videos\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "val_dataset = make_dataset(\n",
    "    cfg['dataset_name'], False, cfg['test_split'], **cfg['dataset']\n",
    ")\n",
    "# set bs = 1, and disable shuffle\n",
    "val_loader = make_data_loader(\n",
    "    val_dataset, False, None, 1, cfg['loader']['num_workers']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(val_loader))\n",
    "sample[0][\"feats\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.tensor([i for i in range(800)])\n",
    "dummy_sample = [{'video_id': '3434_3434',\n",
    "                 'feats': torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]),\n",
    "                 #'feats': test_tensor,\n",
    "                 'segments': torch.tensor([[1.0, 2.0]]),\n",
    "                 'labels': torch.tensor([0]),\n",
    "                 'fps': 12.48324147177119,\n",
    "                 'duration': 33.565,\n",
    "                 'feat_stride': 0.5455729166666666,\n",
    "                 'feat_num_frames': 0.5455729166666666}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = make_meta_arch(cfg['model_name'], **cfg['model'])\n",
    "# only one gpu\n",
    "model = nn.DataParallel(model, device_ids=cfg['devices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'Pretrained/psynd_byola/epoch_015.pth.tar'\n",
      "Loading from EMA model ...\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "print(\"=> loading checkpoint '{}'\".format(ckpt))\n",
    "# load ckpt, reset epoch / best rmse\n",
    "checkpoint = torch.load(\n",
    "    ckpt,\n",
    "    map_location = lambda storage, loc: storage.cuda(cfg['devices'][0])\n",
    ")\n",
    "# load ema model instead\n",
    "print(\"Loading from EMA model ...\")\n",
    "model.load_state_dict(checkpoint['state_dict_ema'])\n",
    "del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAAA0CAIAAAAG8IFEAAAABmJLR0QA/wD/AP+gvaeTAAAErElEQVR4nO2bfUwTZxjA37u2VKgdIF8FyixfCk7qQIeUJdMpccM5M7BsZM6I0wwdjLBkZgFZjJuBLWaKcVNYRLYsc5ANmDgRlOwjEkAlWfjuxA+G7VwZXwL9gLb37o8BHq6l5eEmkry//+65554+97t779revRTGGBFA0PPdwAKGuIND3MHhsxfUanV9ff18tfL4ExAQoFAoHixjFqWlpfPX2AJAqVSydfH/m3ENZz76th5/spIuPBQh1zs4xB0c4g4OcQeHuIND3MEh7uAQd3CIOzjEHRziDg5xB4e4g0PcwSHu4BB3cIg7OMQdHOIODnEHh7iD8z+402uLYs9knho0Tw8Plldvcak41/sggu9rv9tZmhT8pTLkq90pLaoRO/HZwmjvFiedTZAVbwsufmNrfV0PM7nG0n2ifGvsNZUBWPlfOHdnbs+tLfNcm5XqPvn4Eo/9PdB4vCYj487otNeGxhv3XziLoz7/PeV7VWKyqSk7665hpvgswfqafVWVoujCrl1lXcnp/l0H0zr7JhrgydLidrs3H87Vjs9hVzl2x6hVhadQwkfhPlOFR24dWV9d0uG5t3h1EPtpsF5dc06wKTPYm48QX/z8O6F0meo3o+34bMGM5OXY9w8GewsQop0j47wY1cC9qbFAP/HS4ZXMyYZqDfw1MG7dYW1ZZ8uysBdXssqKQ3LaX88vXK0I5LFTGfVAj8FVFjKRyQty9xsd7P4T24rPuhd6ceSuiJhACiGEGN0v32qE0b4y1sHjy8PiQzUXK0YZWxXsfgJ0Q6uYOxv6XNdKfHn2U7HeZKT4Qv5I6boTz25s7hPyhchk0NuMw2EMTdnnj1z3fzc3WEyx4jzXiBjnG429JmhhK+9UwGGMfVrGY43IkQNCiZwWYbNxjBY/6baUt4g2mIxI4CJClNl6HMjY0MW3Ko82Sd67vOGFpdT0dZSHv8jSphvGyIuyvvXMcOoOIYSQg23Q0iUyUXv3Lee3v96xGSFT3a8aN48gP4pmrMchrRgHyl+tKDLKj9Y9E+E+14at7AJ0Q6vFnL0kdL9G79AVxFkav42pOXZTa0JofPhS/k1B8opIoe04QgihkW9+XC8480mtA+MMj13ff77g/qpjP9gSh/s1Or5EJIbK4/a844UpPIdK7v3F+EmnDsrYnbxll37SYcRYdDrmdnjBZxQvtiDlkFKwJm/zzoyf05bXY4r23BSb96GvE0II2YojhMzNlzXouZgdGwR2W8GDt0uKhsbd2g5EtU+EnKRpVzaud53MsAy3XjWEbve2X8sG3LqjfBLD5Ydaqtsi98gn5QkDs/5IzbKaLfZOLH4t0eE4GtdevSLccnqFnwOjhVoS/qk+fIYEc6uq5oZ/0iuLwUOP4+93tDQsdS+u+EClBd/5bWPp6OlYHrV9HfhEYcGMVOW04n0x8VLw5Y773xX8pw7EJfQ2fvzFkNl+8uzgPa0oqpL7ctCypftk7el+eU62xMl+sk24v88iF589DW9yX5ZLeLL0hMr0uVYh/6PAIe7gEHdwiDs4xB0cK/fZaCr/0fexIFAqV7EXKfYcUDInamYemhNFkfmzYMj1Dg5xB4e4g/MPTh/rVBwdKBcAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a computational graph using torchviz\n",
    "model.eval()\n",
    "sample_input = sample \n",
    "output = model(sample_input)  # Forward pass through the model\n",
    "graph = make_dot(output[0]['segments'], params=dict(model.named_parameters()))  # Create the graph\n",
    "# Render the graph and display it in the notebook\n",
    "graph.format = \"png\"  # Set the format to PNG\n",
    "graph.render(\"model_visualization\", format=\"png\", cleanup=True)  # Render the graph and clean up intermediate files\n",
    "display(Image(\"model_visualization.png\"))  # Display the PNG in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 768])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ..., 71.4920, 65.5784, 62.1276],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [54.6784, 37.2710,  7.4400,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000, 23.9287, 64.9352,  ...,  0.8069,  0.2973,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 768])\n",
      "tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# create batched features and masks (B,C,T)\n",
    "# upsamples sequence length to max_sequence length (768)\n",
    "model.eval()\n",
    "#batched_inputs, batched_masks = model.preprocessing([dummy_sample[0], dummy_sample[0]])\n",
    "batched_inputs, batched_masks = model.module.preprocessing(sample)\n",
    "print(batched_inputs.shape)\n",
    "print(batched_inputs)\n",
    "print(batched_masks.shape)\n",
    "print(batched_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 768])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ..., 71.4920, 65.5784, 62.1276],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [54.6784, 37.2710,  7.4400,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000, 23.9287, 64.9352,  ...,  0.8069,  0.2973,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 2048, 768])\n",
      "tensor([[[ 2.2799e-01,  2.0101e-02, -1.2325e-02,  ..., -1.6115e-02,\n",
      "          -4.0751e-02,  2.6922e-01],\n",
      "         [ 2.8743e-05, -3.6082e-04,  4.2548e-04,  ..., -1.1783e-04,\n",
      "           3.8731e-04, -7.2478e-04],\n",
      "         [ 1.0474e-01,  2.3855e-01,  1.6676e-01,  ..., -1.0865e-03,\n",
      "          -8.9013e-04,  1.0170e-02],\n",
      "         ...,\n",
      "         [-8.2776e-02, -8.0712e-02, -1.1329e-01,  ...,  7.3298e-01,\n",
      "           6.5169e-01,  2.6654e-01],\n",
      "         [-2.0958e-04,  4.8181e-03, -9.2649e-04,  ..., -4.1848e-04,\n",
      "           1.5604e-03, -6.0160e-04],\n",
      "         [-2.1519e-04, -2.0102e-03, -3.5669e-04,  ..., -6.1271e-05,\n",
      "          -4.3714e-04, -6.5281e-05]]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>)\n",
      "torch.Size([1, 1])\n",
      "tensor([[4.5149]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "L1 reco vs. input:  tensor(10.8882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "L2 Loss:  tensor(1546.5752, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# DCAE (Deep Convolutional Auto Encoder)\n",
    "# learn robust low-dim latent representation of real!! samples\n",
    "# enhance consistency of real samples in latent space: \n",
    "#       sample-level classifier, which distinguishes the category to which the current feature sequence belongs\n",
    "#       uses focal loss --> deal with class imbalance\n",
    "norm_inputs,reco_result, cls_scores = model.module.interpolator(batched_inputs, batched_masks)\n",
    "print(norm_inputs.shape)\n",
    "print(norm_inputs)\n",
    "print(reco_result.shape)\n",
    "print(reco_result)\n",
    "print(cls_scores.shape)\n",
    "print(cls_scores)\n",
    "l1_loss = torch.nn.L1Loss()(norm_inputs, reco_result)\n",
    "l2_loss = torch.nn.MSELoss()(norm_inputs, reco_result)\n",
    "print('L1 reco vs. input: ', l1_loss)\n",
    "print('L2 Loss: ', l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats: [torch.Size([1, 256, 768]), torch.Size([1, 256, 384]), torch.Size([1, 256, 192]), torch.Size([1, 256, 96]), torch.Size([1, 256, 48]), torch.Size([1, 256, 24])]\n",
      "masks: [torch.Size([1, 1, 768]), torch.Size([1, 1, 384]), torch.Size([1, 1, 192]), torch.Size([1, 1, 96]), torch.Size([1, 1, 48]), torch.Size([1, 1, 24])]\n"
     ]
    }
   ],
   "source": [
    "# CRA (Cross Reconstruction Attention) + downsampling for fpn\n",
    "# directly using reconstruction error is difficult (manipulated very close to real, information carried affects difficulty of reconstruction)\n",
    "# we compute similarity scores between pairs of original and reconstructed features --> use these for the weighted average\n",
    "# Qs: features, Ks: reco-features, V: features\n",
    "feats, masks = model.module.backbone(batched_inputs,norm_inputs, reco_result,batched_masks)\n",
    "print(\"feats: \" + str([tens.shape for tens in feats]))\n",
    "print(\"masks: \" + str([tens.shape for tens in masks]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpn_feats: [torch.Size([1, 256, 768]), torch.Size([1, 256, 384]), torch.Size([1, 256, 192]), torch.Size([1, 256, 96]), torch.Size([1, 256, 48]), torch.Size([1, 256, 24])]\n",
      "fpn_masks: [torch.Size([1, 1, 768]), torch.Size([1, 1, 384]), torch.Size([1, 1, 192]), torch.Size([1, 1, 96]), torch.Size([1, 1, 48]), torch.Size([1, 1, 24])]\n"
     ]
    }
   ],
   "source": [
    "# PCA-FPN (Parallel Cross-Attention Feature Pyramid Network)\n",
    "# feats already contain the different downsampled sizes\n",
    "fpn_feats, fpn_masks = model.module.neck(feats, masks)\n",
    "print(\"fpn_feats: \" + str([tens.shape for tens in fpn_feats]))\n",
    "print(\"fpn_masks: \" + str([tens.shape for tens in fpn_masks]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points: [torch.Size([768, 4]), torch.Size([384, 4]), torch.Size([192, 4]), torch.Size([96, 4]), torch.Size([48, 4]), torch.Size([24, 4])]\n"
     ]
    }
   ],
   "source": [
    "# compute the point coordinate along the FPN\n",
    "# this is used for computing the GT or decode the final results\n",
    "# points: List[T x 4] with length = # fpn levels\n",
    "# (shared across all samples in the mini-batch)\n",
    "points = model.module.point_generator(fpn_feats)\n",
    "print(\"points: \" + str([tens.shape for tens in points]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_cls_logits: [torch.Size([1, 1, 768]), torch.Size([1, 1, 384]), torch.Size([1, 1, 192]), torch.Size([1, 1, 96]), torch.Size([1, 1, 48]), torch.Size([1, 1, 24])]\n",
      "out_offsets: [torch.Size([1, 2, 768]), torch.Size([1, 2, 384]), torch.Size([1, 2, 192]), torch.Size([1, 2, 96]), torch.Size([1, 2, 48]), torch.Size([1, 2, 24])]\n"
     ]
    }
   ],
   "source": [
    "# feature decoding heads\n",
    "# out_cls: List[B, #cls + 1, T_i]\n",
    "out_cls_logits = model.module.cls_head(fpn_feats, fpn_masks)\n",
    "# out_offset: List[B, 2, T_i]\n",
    "out_offsets = model.module.reg_head(fpn_feats, fpn_masks)\n",
    "print(\"out_cls_logits: \" + str([tens.shape for tens in out_cls_logits]))\n",
    "print(\"out_offsets: \" + str([tens.shape for tens in out_offsets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute the outputs\n",
    "# out_cls: F List[B, #cls, T_i] -> F List[B, T_i, #cls]\n",
    "out_cls_logits = [x.permute(0, 2, 1) for x in out_cls_logits]\n",
    "# out_offset: F List[B, 2 (xC), T_i] -> F List[B, T_i, 2 (xC)]\n",
    "out_offsets = [x.permute(0, 2, 1) for x in out_offsets]\n",
    "# fpn_masks: F list[B, 1, T_i] -> F List[B, T_i]\n",
    "fpn_masks = [x.squeeze(1) for x in fpn_masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'video_id': '7729_102255', 'segments': tensor([[6.3763e+00, 1.3796e+01],\n",
      "        [6.3764e+00, 1.3796e+01],\n",
      "        [6.3763e+00, 1.3796e+01],\n",
      "        [1.1267e+01, 1.3854e+01],\n",
      "        [1.3300e+01, 1.3830e+01],\n",
      "        [3.2188e+01, 3.3565e+01],\n",
      "        [1.2663e+01, 1.3841e+01],\n",
      "        [6.3468e+00, 6.6661e+00],\n",
      "        [3.3486e+01, 3.3565e+01],\n",
      "        [-0.0000e+00, 2.1852e-02],\n",
      "        [8.1597e+00, 8.7350e+00],\n",
      "        [2.4670e+01, 2.8358e+01],\n",
      "        [3.9993e+00, 4.2418e+00],\n",
      "        [2.0589e+01, 2.4126e+01],\n",
      "        [6.9641e+00, 7.4834e+00],\n",
      "        [2.1886e+01, 2.2124e+01],\n",
      "        [-0.0000e+00, 1.8608e-01],\n",
      "        [8.7443e+00, 9.3431e+00],\n",
      "        [1.4058e+01, 1.4749e+01],\n",
      "        [2.3765e+01, 2.3997e+01],\n",
      "        [2.7434e+01, 2.7670e+01],\n",
      "        [2.2618e+00, 2.4992e+00],\n",
      "        [3.1587e+01, 3.1823e+01],\n",
      "        [2.5993e+01, 2.6226e+01],\n",
      "        [3.0887e+01, 3.1125e+01],\n",
      "        [2.5648e+01, 2.5880e+01],\n",
      "        [5.4593e+00, 5.6930e+00],\n",
      "        [1.9571e+01, 1.9807e+01],\n",
      "        [1.9265e+01, 1.9498e+01],\n",
      "        [6.0718e+00, 6.3116e+00],\n",
      "        [3.6203e+00, 3.8542e+00],\n",
      "        [1.6558e+01, 1.6788e+01],\n",
      "        [1.7606e+01, 1.7838e+01],\n",
      "        [1.6209e+01, 1.6441e+01],\n",
      "        [4.6246e+00, 4.8590e+00],\n",
      "        [1.0844e+00, 1.3208e+00],\n",
      "        [7.6332e+00, 8.1782e+00],\n",
      "        [2.7047e+01, 2.7282e+01],\n",
      "        [1.5685e+01, 1.5917e+01],\n",
      "        [2.3979e+01, 2.4984e+01],\n",
      "        [2.6654e+01, 2.6887e+01],\n",
      "        [1.5073e+01, 1.5300e+01],\n",
      "        [1.5988e+01, 1.6223e+01],\n",
      "        [3.0950e+00, 3.3288e+00],\n",
      "        [5.1515e+00, 5.3835e+00],\n",
      "        [1.1425e+01, 1.1867e+01],\n",
      "        [2.1497e+01, 2.1726e+01],\n",
      "        [3.0676e+01, 3.0910e+01],\n",
      "        [3.1284e+01, 3.1518e+01],\n",
      "        [1.7871e+01, 1.8103e+01],\n",
      "        [1.9173e+00, 2.1533e+00],\n",
      "        [3.3895e-01, 5.7553e-01],\n",
      "        [1.5425e+01, 1.5652e+01],\n",
      "        [1.4858e+01, 1.5088e+01],\n",
      "        [3.0283e+01, 3.0514e+01],\n",
      "        [2.9496e+01, 2.9729e+01],\n",
      "        [2.8227e+01, 2.8462e+01],\n",
      "        [2.9801e+01, 3.0035e+01],\n",
      "        [1.8874e+01, 1.9104e+01],\n",
      "        [2.1233e+01, 2.1468e+01],\n",
      "        [5.7663e+00, 6.0023e+00],\n",
      "        [1.8306e+01, 1.8541e+01],\n",
      "        [4.4082e+00, 4.6393e+00],\n",
      "        [2.7879e+01, 2.8112e+01],\n",
      "        [2.8664e+01, 2.8900e+01],\n",
      "        [2.3418e+01, 2.3646e+01],\n",
      "        [1.9968e+01, 2.0203e+01],\n",
      "        [1.8613e+01, 1.8846e+01],\n",
      "        [2.0928e+01, 2.1163e+01],\n",
      "        [1.7389e+01, 1.7619e+01],\n",
      "        [2.5166e+01, 2.5392e+01],\n",
      "        [4.8452e+00, 5.0786e+00],\n",
      "        [9.3461e+00, 9.9531e+00],\n",
      "        [1.0418e+01, 1.0882e+01],\n",
      "        [7.3654e-01, 9.7259e-01],\n",
      "        [1.7085e+01, 1.7316e+01],\n",
      "        [2.6610e+00, 2.8947e+00],\n",
      "        [2.2329e+01, 2.2563e+01],\n",
      "        [1.6545e+00, 1.8894e+00],\n",
      "        [3.1854e+01, 3.2090e+01],\n",
      "        [2.0580e+01, 2.0814e+01],\n",
      "        [3.2422e+01, 3.2660e+01],\n",
      "        [2.2984e+01, 2.3217e+01],\n",
      "        [2.9232e+01, 2.9464e+01],\n",
      "        [2.2720e+01, 2.2951e+01],\n",
      "        [3.3079e+01, 3.3314e+01],\n",
      "        [1.0902e+01, 1.1359e+01],\n",
      "        [2.6392e+01, 2.6624e+01],\n",
      "        [2.8927e+01, 2.9161e+01],\n",
      "        [1.3937e+00, 1.6285e+00],\n",
      "        [2.3202e+01, 2.3435e+01],\n",
      "        [3.2816e+01, 3.3052e+01],\n",
      "        [3.0020e+01, 3.0253e+01],\n",
      "        [2.0362e+01, 2.0596e+01],\n",
      "        [-0.0000e+00, 2.1852e-02],\n",
      "        [3.2118e+01, 3.2354e+01],\n",
      "        [3.3145e+00, 3.5404e+00],\n",
      "        [1.6821e+01, 1.7052e+01],\n",
      "        [2.4951e+01, 2.5184e+01],\n",
      "        [2.8446e+01, 2.8681e+01]]), 'scores': tensor([0.9677, 0.2583, 0.0666, 0.0112, 0.0088, 0.0067, 0.0063, 0.0063, 0.0061,\n",
      "        0.0058, 0.0057, 0.0057, 0.0056, 0.0055, 0.0055, 0.0054, 0.0054, 0.0053,\n",
      "        0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0052,\n",
      "        0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052,\n",
      "        0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0051, 0.0051, 0.0051, 0.0051,\n",
      "        0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051,\n",
      "        0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051,\n",
      "        0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0050,\n",
      "        0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "        0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "        0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "        0.0050]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])}]\n"
     ]
    }
   ],
   "source": [
    "results = model.module.inference(\n",
    "            sample, points, fpn_masks,\n",
    "            out_cls_logits, out_offsets\n",
    "        )\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]['segments'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test'] subset has 508 videos\n",
      "torch.Size([4096, 768])\n",
      "=> loading checkpoint 'Pretrained/tvil_tsn/epoch_015.pth.tar'\n",
      "Loading from EMA model ...\n",
      "[{'video_id': 'ec9b46eb6c_result', 'segments': tensor([[0.1688, 0.3971],\n",
      "        [0.1688, 0.3971],\n",
      "        [3.8740, 4.1452],\n",
      "        [0.1688, 0.3971],\n",
      "        [3.3046, 3.6161],\n",
      "        [-0.0000, 0.0934],\n",
      "        [-0.0000, 0.2810],\n",
      "        [3.8744, 4.1451],\n",
      "        [5.7435, 6.0000],\n",
      "        [0.2432, 0.6755],\n",
      "        [3.4482, 4.0720],\n",
      "        [0.5284, 0.7091],\n",
      "        [0.1688, 0.3970],\n",
      "        [2.0002, 2.4928],\n",
      "        [2.8649, 3.3641],\n",
      "        [3.3092, 3.4700],\n",
      "        [3.7638, 4.2073],\n",
      "        [0.3809, 1.3193],\n",
      "        [4.1194, 4.6380],\n",
      "        [5.9846, 6.0000],\n",
      "        [4.7694, 5.1069],\n",
      "        [1.3770, 1.9589],\n",
      "        [0.0071, 0.1391],\n",
      "        [0.4284, 0.5147],\n",
      "        [5.8761, 6.0000],\n",
      "        [5.2817, 5.6061],\n",
      "        [1.1315, 1.4585],\n",
      "        [2.2406, 2.3955],\n",
      "        [3.6455, 3.8121],\n",
      "        [-0.0000, 0.0226],\n",
      "        [2.5958, 2.7388],\n",
      "        [4.6215, 4.7613],\n",
      "        [0.8374, 0.9960],\n",
      "        [3.1265, 3.2721],\n",
      "        [1.7539, 2.2728],\n",
      "        [2.7080, 3.0066],\n",
      "        [2.9829, 3.1432],\n",
      "        [0.3921, 0.4294],\n",
      "        [5.0437, 5.3512],\n",
      "        [0.1177, 0.1559],\n",
      "        [0.7049, 0.7433],\n",
      "        [2.5253, 2.5633],\n",
      "        [5.6188, 5.7012],\n",
      "        [2.5643, 2.6024],\n",
      "        [0.7597, 0.7978],\n",
      "        [2.4160, 2.4540],\n",
      "        [0.7988, 0.8369],\n",
      "        [5.7131, 5.7510],\n",
      "        [2.4784, 2.5165],\n",
      "        [0.9941, 1.0321],\n",
      "        [1.1036, 1.1416],\n",
      "        [1.0567, 1.0947],\n",
      "        [1.9788, 2.0166],\n",
      "        [4.3851, 4.4230],\n",
      "        [4.5414, 4.5793],\n",
      "        [1.6977, 1.7354],\n",
      "        [1.6586, 1.6964],\n",
      "        [1.5023, 1.5402],\n",
      "        [4.1975, 4.2353],\n",
      "        [1.4554, 1.4933],\n",
      "        [1.6039, 1.6417],\n",
      "        [4.4553, 4.4933],\n",
      "        [4.3225, 4.3604],\n",
      "        [1.5414, 1.5792],\n",
      "        [0.5093, 0.5470],\n",
      "        [4.2600, 4.2979],\n",
      "        [5.5803, 5.6182],\n",
      "        [4.7523, 4.7901],\n",
      "        [4.5804, 4.6182],\n",
      "        [1.8694, 1.9073],\n",
      "        [1.9397, 1.9776],\n",
      "        [1.7367, 1.7745],\n",
      "        [3.6119, 3.6498],\n",
      "        [3.2757, 3.3137],\n",
      "        [2.1897, 2.2276],\n",
      "        [2.0255, 2.0634],\n",
      "        [1.8305, 1.8683],\n",
      "        [2.1115, 2.1494],\n",
      "        [1.7758, 1.8136],\n",
      "        [4.5024, 4.5402],\n",
      "        [2.7362, 2.7742],\n",
      "        [4.8852, 4.9230],\n",
      "        [2.0646, 2.1026],\n",
      "        [4.1585, 4.1963],\n",
      "        [2.1506, 2.1885],\n",
      "        [5.3459, 5.3838],\n",
      "        [4.9476, 4.9854],\n",
      "        [4.4162, 4.4542],\n",
      "        [5.3850, 5.4229],\n",
      "        [4.8226, 4.8604],\n",
      "        [5.2444, 5.2823],\n",
      "        [1.1427, 1.1806],\n",
      "        [5.4475, 5.4854],\n",
      "        [5.4866, 5.5245],\n",
      "        [5.1351, 5.1729],\n",
      "        [1.1974, 1.2354],\n",
      "        [2.8378, 2.8758],\n",
      "        [3.8385, 3.8763],\n",
      "        [5.5257, 5.5636],\n",
      "        [1.3069, 1.3448]]), 'scores': tensor([0.8542, 0.2261, 0.0513, 0.0385, 0.0206, 0.0159, 0.0145, 0.0140, 0.0119,\n",
      "        0.0119, 0.0112, 0.0094, 0.0084, 0.0084, 0.0078, 0.0074, 0.0071, 0.0070,\n",
      "        0.0067, 0.0061, 0.0061, 0.0058, 0.0058, 0.0057, 0.0056, 0.0055, 0.0055,\n",
      "        0.0055, 0.0054, 0.0054, 0.0054, 0.0054, 0.0053, 0.0052, 0.0052, 0.0052,\n",
      "        0.0051, 0.0051, 0.0050, 0.0050, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049,\n",
      "        0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049,\n",
      "        0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049,\n",
      "        0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049,\n",
      "        0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049,\n",
      "        0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0048,\n",
      "        0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0048,\n",
      "        0.0048]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])}]\n"
     ]
    }
   ],
   "source": [
    "# apply forward with TVIL data\n",
    "# model to load\n",
    "config='configs/UMMAFormer/tvil_tsn.yaml'\n",
    "ckpt='Pretrained/tvil_tsn/epoch_015.pth.tar'\n",
    "epoch=15\n",
    "_ = fix_random_seed(0, include_cuda=True)\n",
    "# load config\n",
    "if os.path.isfile(config):\n",
    "    cfg = load_config(config)\n",
    "# data\n",
    "val_dataset = make_dataset(\n",
    "    cfg['dataset_name'], False, cfg['test_split'], **cfg['dataset']\n",
    ")\n",
    "# set bs = 1, and disable shuffle\n",
    "val_loader = make_data_loader(\n",
    "    val_dataset, False, None, 1, cfg['loader']['num_workers']\n",
    ")\n",
    "it = iter(val_loader)\n",
    "sample1 = next(it)\n",
    "sample2 = next(it)\n",
    "print(sample[0][\"feats\"].shape)\n",
    "# model\n",
    "model = make_meta_arch(cfg['model_name'], **cfg['model'])\n",
    "# only one gpu\n",
    "model = nn.DataParallel(model, device_ids=cfg['devices'])\n",
    "# load checkpoint\n",
    "print(\"=> loading checkpoint '{}'\".format(ckpt))\n",
    "# load ckpt, reset epoch / best rmse\n",
    "checkpoint = torch.load(\n",
    "    ckpt,\n",
    "    map_location = lambda storage, loc: storage.cuda(cfg['devices'][0])\n",
    ")\n",
    "# load ema model instead\n",
    "print(\"Loading from EMA model ...\")\n",
    "model.load_state_dict(checkpoint['state_dict_ema'])\n",
    "del checkpoint\n",
    "model.eval()\n",
    "# forward pass\n",
    "results = model.module.forward(sample2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"position: relative; width: 640px;\">\n",
       "        <video id=\"vid\" width=\"640\" controls style=\"display: block;\">\n",
       "            <source src=\"data/TVIL/videos/test/ec9b46eb6c_result.mp4\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        <div id=\"progressbar-container\" style=\"position: relative; width: 640px; height: 10px; background: #eee; margin-top: 2px;\">\n",
       "            \n",
       "        <div style=\"\n",
       "            position: absolute;\n",
       "            left: 2.812924385070801%;\n",
       "            width: 3.805095672607422%;\n",
       "            top: 0;\n",
       "            bottom: 0;\n",
       "            background: red;\n",
       "            opacity: 0.5;\n",
       "            pointer-events: none;\n",
       "        \"></div>\n",
       "        \n",
       "            <div id=\"progressbar-current\" style=\"position: absolute; left: 0; top: 0; bottom: 0; width: 0%; background: #2196F3; opacity: 0.8;\"></div>\n",
       "        </div>\n",
       "    </div>\n",
       "    <script>\n",
       "    (function() {\n",
       "        var video = document.getElementById('vid');\n",
       "        var progress = document.getElementById('progressbar-current');\n",
       "        var container = document.getElementById('progressbar-container');\n",
       "        video.addEventListener('timeupdate', function() {\n",
       "            var percent = 100 * video.currentTime / video.duration;\n",
       "            progress.style.width = percent + '%';\n",
       "        });\n",
       "        // Allow clicking on the progress bar to seek\n",
       "        container.addEventListener('click', function(e) {\n",
       "            var rect = container.getBoundingClientRect();\n",
       "            var x = e.clientX - rect.left;\n",
       "            var percent = x / rect.width;\n",
       "            video.currentTime = percent * video.duration;\n",
       "        });\n",
       "    })();\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the sample and results\n",
    "video_id = results[0]['video_id']\n",
    "segments = results[0]['segments'].cpu().numpy()\n",
    "scores = results[0]['scores'].cpu().numpy()\n",
    "tsf.show_vid_with_segments(video_id=video_id, segments=segments, scores=scores, cfg=cfg, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
