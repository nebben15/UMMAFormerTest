{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchviz import make_dot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import eval\n",
    "import test_stuff_functions as tsf\n",
    "\n",
    "from libs.core import load_config\n",
    "from libs.datasets import make_dataset, make_data_loader\n",
    "from libs.modeling import make_meta_arch\n",
    "from libs.utils import fix_random_seed\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'audio_feat_folder': './data/Psynd/feats/byola',\n",
      "             'audio_file_ext': None,\n",
      "             'audio_input_dim': 2048,\n",
      "             'crop_ratio': [0.9, 1.0],\n",
      "             'default_fps': None,\n",
      "             'downsample_rate': 1,\n",
      "             'feat_stride': 1,\n",
      "             'file_ext': '.npy',\n",
      "             'file_prefix': 'none',\n",
      "             'force_upsampling': True,\n",
      "             'input_dim': 0,\n",
      "             'json_file': './data/Psynd/annotations/metadatabyola.json',\n",
      "             'max_seq_len': 768,\n",
      "             'num_classes': 1,\n",
      "             'num_frames': 1,\n",
      "             'trunc_thresh': 0.5},\n",
      " 'dataset_name': 'psynd',\n",
      " 'devices': [0],\n",
      " 'init_rand_seed': 1234567891,\n",
      " 'loader': {'batch_size': 8, 'num_workers': 4},\n",
      " 'model': {'audio_input_dim': 2048,\n",
      "           'backbone_arch': (2, 2, 5),\n",
      "           'backbone_type': 'convHRLRFullResSelfAttTransformerRevised',\n",
      "           'embd_dim': 256,\n",
      "           'embd_kernel_size': 3,\n",
      "           'embd_with_ln': True,\n",
      "           'fpn_dim': 256,\n",
      "           'fpn_start_level': 0,\n",
      "           'fpn_type': 'fpn',\n",
      "           'fpn_with_ln': True,\n",
      "           'head_dim': 256,\n",
      "           'head_kernel_size': 3,\n",
      "           'head_num_layers': 3,\n",
      "           'head_with_ln': True,\n",
      "           'input_dim': 0,\n",
      "           'max_buffer_len_factor': 1.0,\n",
      "           'max_seq_len': 768,\n",
      "           'n_head': 4,\n",
      "           'n_mha_win_size': [7, 7, 7, 7, 7, -1],\n",
      "           'num_classes': 1,\n",
      "           'regression_range': [(0, 4),\n",
      "                                (4, 8),\n",
      "                                (8, 16),\n",
      "                                (16, 32),\n",
      "                                (32, 64),\n",
      "                                (64, 10000)],\n",
      "           'scale_factor': 2,\n",
      "           'test_cfg': {'duration_thresh': 0.001,\n",
      "                        'ext_score_file': None,\n",
      "                        'iou_threshold': 0.1,\n",
      "                        'max_seg_num': 100,\n",
      "                        'min_score': 0.001,\n",
      "                        'multiclass_nms': False,\n",
      "                        'nms_method': 'soft',\n",
      "                        'nms_sigma': 0.75,\n",
      "                        'pre_nms_thresh': 0.001,\n",
      "                        'pre_nms_topk': 2000,\n",
      "                        'voting_thresh': 0.9},\n",
      "           'train_cfg': {'center_sample': 'radius',\n",
      "                         'center_sample_radius': 1.5,\n",
      "                         'clip_grad_l2norm': 1.0,\n",
      "                         'cls_prior_prob': 0.01,\n",
      "                         'dropout': 0.0,\n",
      "                         'droppath': 0.1,\n",
      "                         'head_empty_cls': [],\n",
      "                         'init_loss_norm': 200,\n",
      "                         'label_smoothing': 0.1,\n",
      "                         'loss_weight': 2.0},\n",
      "           'use_abs_pe': True,\n",
      "           'use_rel_pe': False},\n",
      " 'model_name': 'AVLocPointTransformerRecoveryNoNorm',\n",
      " 'opt': {'epochs': 10,\n",
      "         'learning_rate': 0.001,\n",
      "         'momentum': 0.9,\n",
      "         'schedule_gamma': 0.1,\n",
      "         'schedule_steps': [],\n",
      "         'schedule_type': 'cosine',\n",
      "         'type': 'AdamW',\n",
      "         'warmup': True,\n",
      "         'warmup_epochs': 5,\n",
      "         'weight_decay': 0.05},\n",
      " 'output_folder': './paper_results/Revised',\n",
      " 'test_cfg': {'duration_thresh': 0.001,\n",
      "              'ext_score_file': None,\n",
      "              'iou_threshold': 0.1,\n",
      "              'max_seg_num': 100,\n",
      "              'min_score': 0.001,\n",
      "              'multiclass_nms': False,\n",
      "              'nms_method': 'soft',\n",
      "              'nms_sigma': 0.75,\n",
      "              'pre_nms_thresh': 0.001,\n",
      "              'pre_nms_topk': 2000,\n",
      "              'voting_thresh': 0.9},\n",
      " 'test_split': ['test'],\n",
      " 'train_cfg': {'center_sample': 'radius',\n",
      "               'center_sample_radius': 1.5,\n",
      "               'clip_grad_l2norm': 1.0,\n",
      "               'cls_prior_prob': 0.01,\n",
      "               'dropout': 0.0,\n",
      "               'droppath': 0.1,\n",
      "               'head_empty_cls': [],\n",
      "               'init_loss_norm': 200,\n",
      "               'label_smoothing': 0.1,\n",
      "               'loss_weight': 2.0},\n",
      " 'train_split': ['train'],\n",
      " 'val_split': ['dev']}\n",
      "['test'] subset has 79 videos\n",
      "=> loading checkpoint 'Pretrained/psynd_byola/epoch_015.pth.tar'\n",
      "Loading from EMA model ...\n",
      "\n",
      "Start testing model AVLocPointTransformerRecoveryNoNorm ...\n",
      "Test: [00010/00079]\tTime 0.17 (0.17)\n",
      "Test: [00020/00079]\tTime 0.14 (0.15)\n",
      "Test: [00030/00079]\tTime 0.13 (0.15)\n",
      "Test: [00040/00079]\tTime 0.13 (0.14)\n",
      "Test: [00050/00079]\tTime 0.14 (0.14)\n",
      "Test: [00060/00079]\tTime 0.13 (0.14)\n",
      "Test: [00070/00079]\tTime 0.13 (0.14)\n",
      "saving detection results...\n",
      "evaluion detection results...\n",
      "{'Fake': 0}\n",
      "[INIT] Loaded annotations from test subset.\n",
      "\tNumber of ground truth instances: 79\n",
      "\tNumber of predictions: 7900\n",
      "\tFixed threshold for tiou score: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "[RESULTS] Performance on  detection task.\n",
      "Average-mAP: 0.9697027938387699\n",
      "Detection: average-mAP 96.970 mAP@0.50 100.000 mAP@0.55 100.000 mAP@0.60 100.000 mAP@0.65 100.000 mAP@0.70 100.000 mAP@0.75 98.574 mAP@0.80 98.574 mAP@0.85 97.322 mAP@0.90 95.363 mAP@0.95 79.870\n",
      "evaluion proposal results...\n",
      "{'Fake': 0}\n",
      "[INIT] Loaded annotations from test subset.\n",
      "\tNumber of ground truth instances: 79\n",
      "\tNumber of proposals: 7900\n",
      "\tFixed threshold for tiou score: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "[RESULTS] Performance on  proposal task.\n",
      "\tArea Under the AR vs AN curve: 96.6177215189873%\n",
      "All done! Total time: 21.01 sec\n"
     ]
    }
   ],
   "source": [
    "eval.run(config='configs/UMMAFormer/psynd_byola.yaml', ckpt='Pretrained/psynd_byola/epoch_015.pth.tar', epoch=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Results:\n",
      "   average-mAP  mAP@0.50  mAP@0.55  mAP@0.60  mAP@0.65  mAP@0.70  mAP@0.75  \\\n",
      "0        96.97     100.0     100.0     100.0     100.0     100.0    98.574   \n",
      "\n",
      "   mAP@0.80  mAP@0.85  mAP@0.90  mAP@0.95  \n",
      "0    98.574    97.322    95.363     79.87  \n",
      "\n",
      "Proposal Results:\n",
      "    AR@10   AR@20   AR@50  AR@100\n",
      "0  97.595  97.595  97.595  97.595\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/ben/Thesis/UMMAFormerTest/Pretrained/psynd_byola/test_results.txt'\n",
    "detection_df, proposal_df = tsf.format_results_as_matrix(file_path)\n",
    "\n",
    "# Display the matrices\n",
    "print(\"Detection Results:\")\n",
    "print(detection_df)\n",
    "\n",
    "print(\"\\nProposal Results:\")\n",
    "print(proposal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to load\n",
    "config='configs/UMMAFormer/psynd_byola.yaml'\n",
    "ckpt='Pretrained/psynd_byola/epoch_015.pth.tar'\n",
    "epoch=15\n",
    "_ = fix_random_seed(0, include_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "if os.path.isfile(config):\n",
    "    cfg = load_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test'] subset has 79 videos\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "val_dataset = make_dataset(\n",
    "    cfg['dataset_name'], False, cfg['test_split'], **cfg['dataset']\n",
    ")\n",
    "# set bs = 1, and disable shuffle\n",
    "val_loader = make_data_loader(\n",
    "    val_dataset, False, None, 1, cfg['loader']['num_workers']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(val_loader))\n",
    "sample[0][\"feats\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = make_meta_arch(cfg['model_name'], **cfg['model'])\n",
    "# only one gpu\n",
    "#model = nn.DataParallel(model, device_ids=cfg['devices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAAA0CAIAAAAG8IFEAAAABmJLR0QA/wD/AP+gvaeTAAAErElEQVR4nO2bfUwTZxjA37u2VKgdIF8FyixfCk7qQIeUJdMpccM5M7BsZM6I0wwdjLBkZgFZjJuBLWaKcVNYRLYsc5ANmDgRlOwjEkAlWfjuxA+G7VwZXwL9gLb37o8BHq6l5eEmkry//+65554+97t779revRTGGBFA0PPdwAKGuIND3MHhsxfUanV9ff18tfL4ExAQoFAoHixjFqWlpfPX2AJAqVSydfH/m3ENZz76th5/spIuPBQh1zs4xB0c4g4OcQeHuIND3MEh7uAQd3CIOzjEHRziDg5xB4e4g0PcwSHu4BB3cIg7OMQdHOIODnEHh7iD8z+402uLYs9knho0Tw8Plldvcak41/sggu9rv9tZmhT8pTLkq90pLaoRO/HZwmjvFiedTZAVbwsufmNrfV0PM7nG0n2ifGvsNZUBWPlfOHdnbs+tLfNcm5XqPvn4Eo/9PdB4vCYj487otNeGxhv3XziLoz7/PeV7VWKyqSk7665hpvgswfqafVWVoujCrl1lXcnp/l0H0zr7JhrgydLidrs3H87Vjs9hVzl2x6hVhadQwkfhPlOFR24dWV9d0uG5t3h1EPtpsF5dc06wKTPYm48QX/z8O6F0meo3o+34bMGM5OXY9w8GewsQop0j47wY1cC9qbFAP/HS4ZXMyYZqDfw1MG7dYW1ZZ8uysBdXssqKQ3LaX88vXK0I5LFTGfVAj8FVFjKRyQty9xsd7P4T24rPuhd6ceSuiJhACiGEGN0v32qE0b4y1sHjy8PiQzUXK0YZWxXsfgJ0Q6uYOxv6XNdKfHn2U7HeZKT4Qv5I6boTz25s7hPyhchk0NuMw2EMTdnnj1z3fzc3WEyx4jzXiBjnG429JmhhK+9UwGGMfVrGY43IkQNCiZwWYbNxjBY/6baUt4g2mIxI4CJClNl6HMjY0MW3Ko82Sd67vOGFpdT0dZSHv8jSphvGyIuyvvXMcOoOIYSQg23Q0iUyUXv3Lee3v96xGSFT3a8aN48gP4pmrMchrRgHyl+tKDLKj9Y9E+E+14at7AJ0Q6vFnL0kdL9G79AVxFkav42pOXZTa0JofPhS/k1B8opIoe04QgihkW9+XC8480mtA+MMj13ff77g/qpjP9gSh/s1Or5EJIbK4/a844UpPIdK7v3F+EmnDsrYnbxll37SYcRYdDrmdnjBZxQvtiDlkFKwJm/zzoyf05bXY4r23BSb96GvE0II2YojhMzNlzXouZgdGwR2W8GDt0uKhsbd2g5EtU+EnKRpVzaud53MsAy3XjWEbve2X8sG3LqjfBLD5Ydaqtsi98gn5QkDs/5IzbKaLfZOLH4t0eE4GtdevSLccnqFnwOjhVoS/qk+fIYEc6uq5oZ/0iuLwUOP4+93tDQsdS+u+EClBd/5bWPp6OlYHrV9HfhEYcGMVOW04n0x8VLw5Y773xX8pw7EJfQ2fvzFkNl+8uzgPa0oqpL7ctCypftk7el+eU62xMl+sk24v88iF589DW9yX5ZLeLL0hMr0uVYh/6PAIe7gEHdwiDs4xB0cK/fZaCr/0fexIFAqV7EXKfYcUDInamYemhNFkfmzYMj1Dg5xB4e4g/MPTh/rVBwdKBcAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a computational graph using torchviz\n",
    "model.eval()\n",
    "sample_input = sample \n",
    "output = model(sample_input)  # Forward pass through the model\n",
    "graph = make_dot(output[0]['segments'], params=dict(model.named_parameters()))  # Create the graph\n",
    "# Render the graph and display it in the notebook\n",
    "graph.format = \"png\"  # Set the format to PNG\n",
    "graph.render(\"model_visualization\", format=\"png\", cleanup=True)  # Render the graph and clean up intermediate files\n",
    "display(Image(\"model_visualization.png\"))  # Display the PNG in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
