{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchviz import make_dot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import eval\n",
    "import train\n",
    "import test_stuff_functions as tsf\n",
    "\n",
    "from libs.core import load_config\n",
    "from libs.datasets import make_dataset, make_data_loader\n",
    "from libs.modeling import make_meta_arch\n",
    "from libs.utils import fix_random_seed\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'audio_feat_folder': './data/Psynd/feats/byola',\n",
      "             'audio_file_ext': None,\n",
      "             'audio_input_dim': 2048,\n",
      "             'crop_ratio': [0.9, 1.0],\n",
      "             'default_fps': None,\n",
      "             'downsample_rate': 1,\n",
      "             'feat_stride': 1,\n",
      "             'file_ext': '.npy',\n",
      "             'file_prefix': 'none',\n",
      "             'force_upsampling': True,\n",
      "             'input_dim': 0,\n",
      "             'json_file': './data/Psynd/annotations/metadatabyola.json',\n",
      "             'max_seq_len': 768,\n",
      "             'num_classes': 1,\n",
      "             'num_frames': 1,\n",
      "             'trunc_thresh': 0.5},\n",
      " 'dataset_name': 'psynd',\n",
      " 'devices': [0],\n",
      " 'init_rand_seed': 1234567891,\n",
      " 'loader': {'batch_size': 8, 'num_workers': 4},\n",
      " 'model': {'audio_input_dim': 2048,\n",
      "           'backbone_arch': (2, 2, 5),\n",
      "           'backbone_type': 'convHRLRFullResSelfAttTransformerRevised',\n",
      "           'embd_dim': 256,\n",
      "           'embd_kernel_size': 3,\n",
      "           'embd_with_ln': True,\n",
      "           'fpn_dim': 256,\n",
      "           'fpn_start_level': 0,\n",
      "           'fpn_type': 'fpn',\n",
      "           'fpn_with_ln': True,\n",
      "           'head_dim': 256,\n",
      "           'head_kernel_size': 3,\n",
      "           'head_num_layers': 3,\n",
      "           'head_with_ln': True,\n",
      "           'input_dim': 0,\n",
      "           'max_buffer_len_factor': 1.0,\n",
      "           'max_seq_len': 768,\n",
      "           'n_head': 4,\n",
      "           'n_mha_win_size': [7, 7, 7, 7, 7, -1],\n",
      "           'num_classes': 1,\n",
      "           'regression_range': [(0, 4),\n",
      "                                (4, 8),\n",
      "                                (8, 16),\n",
      "                                (16, 32),\n",
      "                                (32, 64),\n",
      "                                (64, 10000)],\n",
      "           'scale_factor': 2,\n",
      "           'test_cfg': {'duration_thresh': 0.001,\n",
      "                        'ext_score_file': None,\n",
      "                        'iou_threshold': 0.1,\n",
      "                        'max_seg_num': 100,\n",
      "                        'min_score': 0.001,\n",
      "                        'multiclass_nms': False,\n",
      "                        'nms_method': 'soft',\n",
      "                        'nms_sigma': 0.75,\n",
      "                        'pre_nms_thresh': 0.001,\n",
      "                        'pre_nms_topk': 2000,\n",
      "                        'voting_thresh': 0.9},\n",
      "           'train_cfg': {'center_sample': 'radius',\n",
      "                         'center_sample_radius': 1.5,\n",
      "                         'clip_grad_l2norm': 1.0,\n",
      "                         'cls_prior_prob': 0.01,\n",
      "                         'dropout': 0.0,\n",
      "                         'droppath': 0.1,\n",
      "                         'head_empty_cls': [],\n",
      "                         'init_loss_norm': 200,\n",
      "                         'label_smoothing': 0.1,\n",
      "                         'loss_weight': 2.0},\n",
      "           'use_abs_pe': True,\n",
      "           'use_rel_pe': False},\n",
      " 'model_name': 'AVLocPointTransformerRecoveryNoNorm',\n",
      " 'opt': {'epochs': 10,\n",
      "         'learning_rate': 0.001,\n",
      "         'momentum': 0.9,\n",
      "         'schedule_gamma': 0.1,\n",
      "         'schedule_steps': [],\n",
      "         'schedule_type': 'cosine',\n",
      "         'type': 'AdamW',\n",
      "         'warmup': True,\n",
      "         'warmup_epochs': 5,\n",
      "         'weight_decay': 0.05},\n",
      " 'output_folder': './paper_results/Revised',\n",
      " 'test_cfg': {'duration_thresh': 0.001,\n",
      "              'ext_score_file': None,\n",
      "              'iou_threshold': 0.1,\n",
      "              'max_seg_num': 100,\n",
      "              'min_score': 0.001,\n",
      "              'multiclass_nms': False,\n",
      "              'nms_method': 'soft',\n",
      "              'nms_sigma': 0.75,\n",
      "              'pre_nms_thresh': 0.001,\n",
      "              'pre_nms_topk': 2000,\n",
      "              'voting_thresh': 0.9},\n",
      " 'test_split': ['test'],\n",
      " 'train_cfg': {'center_sample': 'radius',\n",
      "               'center_sample_radius': 1.5,\n",
      "               'clip_grad_l2norm': 1.0,\n",
      "               'cls_prior_prob': 0.01,\n",
      "               'dropout': 0.0,\n",
      "               'droppath': 0.1,\n",
      "               'head_empty_cls': [],\n",
      "               'init_loss_norm': 200,\n",
      "               'label_smoothing': 0.1,\n",
      "               'loss_weight': 2.0},\n",
      " 'train_split': ['train'],\n",
      " 'val_split': ['dev']}\n",
      "['test'] subset has 79 videos\n",
      "=> loading checkpoint 'Pretrained/psynd_byola/epoch_015.pth.tar'\n",
      "Loading from EMA model ...\n",
      "\n",
      "Start testing model AVLocPointTransformerRecoveryNoNorm ...\n",
      "Test: [00010/00079]\tTime 0.17 (0.17)\n",
      "Test: [00020/00079]\tTime 0.14 (0.15)\n",
      "Test: [00030/00079]\tTime 0.13 (0.15)\n",
      "Test: [00040/00079]\tTime 0.13 (0.14)\n",
      "Test: [00050/00079]\tTime 0.14 (0.14)\n",
      "Test: [00060/00079]\tTime 0.13 (0.14)\n",
      "Test: [00070/00079]\tTime 0.13 (0.14)\n",
      "saving detection results...\n",
      "evaluion detection results...\n",
      "{'Fake': 0}\n",
      "[INIT] Loaded annotations from test subset.\n",
      "\tNumber of ground truth instances: 79\n",
      "\tNumber of predictions: 7900\n",
      "\tFixed threshold for tiou score: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "[RESULTS] Performance on  detection task.\n",
      "Average-mAP: 0.9697027938387699\n",
      "Detection: average-mAP 96.970 mAP@0.50 100.000 mAP@0.55 100.000 mAP@0.60 100.000 mAP@0.65 100.000 mAP@0.70 100.000 mAP@0.75 98.574 mAP@0.80 98.574 mAP@0.85 97.322 mAP@0.90 95.363 mAP@0.95 79.870\n",
      "evaluion proposal results...\n",
      "{'Fake': 0}\n",
      "[INIT] Loaded annotations from test subset.\n",
      "\tNumber of ground truth instances: 79\n",
      "\tNumber of proposals: 7900\n",
      "\tFixed threshold for tiou score: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "[RESULTS] Performance on  proposal task.\n",
      "\tArea Under the AR vs AN curve: 96.6177215189873%\n",
      "All done! Total time: 21.01 sec\n"
     ]
    }
   ],
   "source": [
    "eval.run(config='configs/UMMAFormer/psynd_byola.yaml', ckpt='Pretrained/psynd_byola/epoch_015.pth.tar', epoch=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Results:\n",
      "   average-mAP  mAP@0.50  mAP@0.55  mAP@0.60  mAP@0.65  mAP@0.70  mAP@0.75  \\\n",
      "0        96.97     100.0     100.0     100.0     100.0     100.0    98.574   \n",
      "\n",
      "   mAP@0.80  mAP@0.85  mAP@0.90  mAP@0.95  \n",
      "0    98.574    97.322    95.363     79.87  \n",
      "\n",
      "Proposal Results:\n",
      "    AR@10   AR@20   AR@50  AR@100\n",
      "0  97.595  97.595  97.595  97.595\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/ben/Thesis/UMMAFormerTest/Pretrained/psynd_byola/test_results.txt'\n",
    "detection_df, proposal_df = tsf.format_results_as_matrix(file_path)\n",
    "\n",
    "# Display the matrices\n",
    "print(\"Detection Results:\")\n",
    "print(detection_df)\n",
    "\n",
    "print(\"\\nProposal Results:\")\n",
    "print(proposal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to load\n",
    "config='configs/UMMAFormer/psynd_byola.yaml'\n",
    "ckpt='Pretrained/psynd_byola/epoch_015.pth.tar'\n",
    "epoch=15\n",
    "_ = fix_random_seed(0, include_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "if os.path.isfile(config):\n",
    "    cfg = load_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test'] subset has 79 videos\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "val_dataset = make_dataset(\n",
    "    cfg['dataset_name'], False, cfg['test_split'], **cfg['dataset']\n",
    ")\n",
    "# set bs = 1, and disable shuffle\n",
    "val_loader = make_data_loader(\n",
    "    val_dataset, False, None, 1, cfg['loader']['num_workers']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(val_loader))\n",
    "sample[0][\"feats\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.tensor([i for i in range(800)])\n",
    "dummy_sample = [{'video_id': '3434_3434',\n",
    "                 'feats': torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]),\n",
    "                 #'feats': test_tensor,\n",
    "                 'segments': torch.tensor([[1.0, 2.0]]),\n",
    "                 'labels': torch.tensor([0]),\n",
    "                 'fps': 12.48324147177119,\n",
    "                 'duration': 33.565,\n",
    "                 'feat_stride': 0.5455729166666666,\n",
    "                 'feat_num_frames': 0.5455729166666666}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = make_meta_arch(cfg['model_name'], **cfg['model'])\n",
    "# only one gpu\n",
    "model = nn.DataParallel(model, device_ids=cfg['devices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'Pretrained/psynd_byola/epoch_015.pth.tar'\n",
      "Loading from EMA model ...\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "print(\"=> loading checkpoint '{}'\".format(ckpt))\n",
    "# load ckpt, reset epoch / best rmse\n",
    "checkpoint = torch.load(\n",
    "    ckpt,\n",
    "    map_location = lambda storage, loc: storage.cuda(cfg['devices'][0])\n",
    ")\n",
    "# load ema model instead\n",
    "print(\"Loading from EMA model ...\")\n",
    "model.load_state_dict(checkpoint['state_dict_ema'])\n",
    "del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAAA0CAIAAAAG8IFEAAAABmJLR0QA/wD/AP+gvaeTAAAErElEQVR4nO2bfUwTZxjA37u2VKgdIF8FyixfCk7qQIeUJdMpccM5M7BsZM6I0wwdjLBkZgFZjJuBLWaKcVNYRLYsc5ANmDgRlOwjEkAlWfjuxA+G7VwZXwL9gLb37o8BHq6l5eEmkry//+65554+97t779revRTGGBFA0PPdwAKGuIND3MHhsxfUanV9ff18tfL4ExAQoFAoHixjFqWlpfPX2AJAqVSydfH/m3ENZz76th5/spIuPBQh1zs4xB0c4g4OcQeHuIND3MEh7uAQd3CIOzjEHRziDg5xB4e4g0PcwSHu4BB3cIg7OMQdHOIODnEHh7iD8z+402uLYs9knho0Tw8Plldvcak41/sggu9rv9tZmhT8pTLkq90pLaoRO/HZwmjvFiedTZAVbwsufmNrfV0PM7nG0n2ifGvsNZUBWPlfOHdnbs+tLfNcm5XqPvn4Eo/9PdB4vCYj487otNeGxhv3XziLoz7/PeV7VWKyqSk7665hpvgswfqafVWVoujCrl1lXcnp/l0H0zr7JhrgydLidrs3H87Vjs9hVzl2x6hVhadQwkfhPlOFR24dWV9d0uG5t3h1EPtpsF5dc06wKTPYm48QX/z8O6F0meo3o+34bMGM5OXY9w8GewsQop0j47wY1cC9qbFAP/HS4ZXMyYZqDfw1MG7dYW1ZZ8uysBdXssqKQ3LaX88vXK0I5LFTGfVAj8FVFjKRyQty9xsd7P4T24rPuhd6ceSuiJhACiGEGN0v32qE0b4y1sHjy8PiQzUXK0YZWxXsfgJ0Q6uYOxv6XNdKfHn2U7HeZKT4Qv5I6boTz25s7hPyhchk0NuMw2EMTdnnj1z3fzc3WEyx4jzXiBjnG429JmhhK+9UwGGMfVrGY43IkQNCiZwWYbNxjBY/6baUt4g2mIxI4CJClNl6HMjY0MW3Ko82Sd67vOGFpdT0dZSHv8jSphvGyIuyvvXMcOoOIYSQg23Q0iUyUXv3Lee3v96xGSFT3a8aN48gP4pmrMchrRgHyl+tKDLKj9Y9E+E+14at7AJ0Q6vFnL0kdL9G79AVxFkav42pOXZTa0JofPhS/k1B8opIoe04QgihkW9+XC8480mtA+MMj13ff77g/qpjP9gSh/s1Or5EJIbK4/a844UpPIdK7v3F+EmnDsrYnbxll37SYcRYdDrmdnjBZxQvtiDlkFKwJm/zzoyf05bXY4r23BSb96GvE0II2YojhMzNlzXouZgdGwR2W8GDt0uKhsbd2g5EtU+EnKRpVzaud53MsAy3XjWEbve2X8sG3LqjfBLD5Ydaqtsi98gn5QkDs/5IzbKaLfZOLH4t0eE4GtdevSLccnqFnwOjhVoS/qk+fIYEc6uq5oZ/0iuLwUOP4+93tDQsdS+u+EClBd/5bWPp6OlYHrV9HfhEYcGMVOW04n0x8VLw5Y773xX8pw7EJfQ2fvzFkNl+8uzgPa0oqpL7ctCypftk7el+eU62xMl+sk24v88iF589DW9yX5ZLeLL0hMr0uVYh/6PAIe7gEHdwiDs4xB0cK/fZaCr/0fexIFAqV7EXKfYcUDInamYemhNFkfmzYMj1Dg5xB4e4g/MPTh/rVBwdKBcAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a computational graph using torchviz\n",
    "model.eval()\n",
    "sample_input = sample \n",
    "output = model(sample_input)  # Forward pass through the model\n",
    "graph = make_dot(output[0]['segments'], params=dict(model.named_parameters()))  # Create the graph\n",
    "# Render the graph and display it in the notebook\n",
    "graph.format = \"png\"  # Set the format to PNG\n",
    "graph.render(\"model_visualization\", format=\"png\", cleanup=True)  # Render the graph and clean up intermediate files\n",
    "display(Image(\"model_visualization.png\"))  # Display the PNG in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 768])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ..., 71.4920, 65.5784, 62.1276],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [54.6784, 37.2710,  7.4400,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000, 23.9287, 64.9352,  ...,  0.8069,  0.2973,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 768])\n",
      "tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# create batched features and masks (B,C,T)\n",
    "# upsamples sequence length to max_sequence length (768)\n",
    "model.eval()\n",
    "#batched_inputs, batched_masks = model.preprocessing([dummy_sample[0], dummy_sample[0]])\n",
    "batched_inputs, batched_masks = model.module.preprocessing(sample)\n",
    "print(batched_inputs.shape)\n",
    "print(batched_inputs)\n",
    "print(batched_masks.shape)\n",
    "print(batched_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 768])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ..., 71.4920, 65.5784, 62.1276],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [54.6784, 37.2710,  7.4400,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000, 23.9287, 64.9352,  ...,  0.8069,  0.2973,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 2048, 768])\n",
      "tensor([[[ 2.2799e-01,  2.0101e-02, -1.2325e-02,  ..., -1.6115e-02,\n",
      "          -4.0751e-02,  2.6922e-01],\n",
      "         [ 2.8743e-05, -3.6082e-04,  4.2548e-04,  ..., -1.1783e-04,\n",
      "           3.8731e-04, -7.2478e-04],\n",
      "         [ 1.0474e-01,  2.3855e-01,  1.6676e-01,  ..., -1.0865e-03,\n",
      "          -8.9013e-04,  1.0170e-02],\n",
      "         ...,\n",
      "         [-8.2776e-02, -8.0712e-02, -1.1329e-01,  ...,  7.3298e-01,\n",
      "           6.5169e-01,  2.6654e-01],\n",
      "         [-2.0958e-04,  4.8181e-03, -9.2649e-04,  ..., -4.1848e-04,\n",
      "           1.5604e-03, -6.0160e-04],\n",
      "         [-2.1519e-04, -2.0102e-03, -3.5669e-04,  ..., -6.1271e-05,\n",
      "          -4.3714e-04, -6.5281e-05]]], device='cuda:0',\n",
      "       grad_fn=<LeakyReluBackward0>)\n",
      "torch.Size([1, 1])\n",
      "tensor([[4.5149]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "L1 reco vs. input:  tensor(10.8882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "L2 Loss:  tensor(1546.5752, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# DCAE (Deep Convolutional Auto Encoder)\n",
    "# learn robust low-dim latent representation of real!! samples\n",
    "# enhance consistency of real samples in latent space: \n",
    "#       sample-level classifier, which distinguishes the category to which the current feature sequence belongs\n",
    "#       uses focal loss --> deal with class imbalance\n",
    "norm_inputs,reco_result, cls_scores = model.module.interpolator(batched_inputs, batched_masks)\n",
    "print(norm_inputs.shape)\n",
    "print(norm_inputs)\n",
    "print(reco_result.shape)\n",
    "print(reco_result)\n",
    "print(cls_scores.shape)\n",
    "print(cls_scores)\n",
    "l1_loss = torch.nn.L1Loss()(norm_inputs, reco_result)\n",
    "l2_loss = torch.nn.MSELoss()(norm_inputs, reco_result)\n",
    "print('L1 reco vs. input: ', l1_loss)\n",
    "print('L2 Loss: ', l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats: [torch.Size([1, 256, 768]), torch.Size([1, 256, 384]), torch.Size([1, 256, 192]), torch.Size([1, 256, 96]), torch.Size([1, 256, 48]), torch.Size([1, 256, 24])]\n",
      "masks: [torch.Size([1, 1, 768]), torch.Size([1, 1, 384]), torch.Size([1, 1, 192]), torch.Size([1, 1, 96]), torch.Size([1, 1, 48]), torch.Size([1, 1, 24])]\n"
     ]
    }
   ],
   "source": [
    "# CRA (Cross Reconstruction Attention) + downsampling for fpn\n",
    "# directly using reconstruction error is difficult (manipulated very close to real, information carried affects difficulty of reconstruction)\n",
    "# we compute similarity scores between pairs of original and reconstructed features --> use these for the weighted average\n",
    "# Qs: features, Ks: reco-features, V: features\n",
    "feats, masks = model.module.backbone(batched_inputs,norm_inputs, reco_result,batched_masks)\n",
    "print(\"feats: \" + str([tens.shape for tens in feats]))\n",
    "print(\"masks: \" + str([tens.shape for tens in masks]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpn_feats: [torch.Size([1, 256, 768]), torch.Size([1, 256, 384]), torch.Size([1, 256, 192]), torch.Size([1, 256, 96]), torch.Size([1, 256, 48]), torch.Size([1, 256, 24])]\n",
      "fpn_masks: [torch.Size([1, 1, 768]), torch.Size([1, 1, 384]), torch.Size([1, 1, 192]), torch.Size([1, 1, 96]), torch.Size([1, 1, 48]), torch.Size([1, 1, 24])]\n"
     ]
    }
   ],
   "source": [
    "# PCA-FPN (Parallel Cross-Attention Feature Pyramid Network)\n",
    "# feats already contain the different downsampled sizes\n",
    "fpn_feats, fpn_masks = model.module.neck(feats, masks)\n",
    "print(\"fpn_feats: \" + str([tens.shape for tens in fpn_feats]))\n",
    "print(\"fpn_masks: \" + str([tens.shape for tens in fpn_masks]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points: [torch.Size([768, 4]), torch.Size([384, 4]), torch.Size([192, 4]), torch.Size([96, 4]), torch.Size([48, 4]), torch.Size([24, 4])]\n"
     ]
    }
   ],
   "source": [
    "# compute the point coordinate along the FPN\n",
    "# this is used for computing the GT or decode the final results\n",
    "# points: List[T x 4] with length = # fpn levels\n",
    "# (shared across all samples in the mini-batch)\n",
    "points = model.module.point_generator(fpn_feats)\n",
    "print(\"points: \" + str([tens.shape for tens in points]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_cls_logits: [torch.Size([1, 1, 768]), torch.Size([1, 1, 384]), torch.Size([1, 1, 192]), torch.Size([1, 1, 96]), torch.Size([1, 1, 48]), torch.Size([1, 1, 24])]\n",
      "out_offsets: [torch.Size([1, 2, 768]), torch.Size([1, 2, 384]), torch.Size([1, 2, 192]), torch.Size([1, 2, 96]), torch.Size([1, 2, 48]), torch.Size([1, 2, 24])]\n"
     ]
    }
   ],
   "source": [
    "# feature decoding heads\n",
    "# out_cls: List[B, #cls + 1, T_i]\n",
    "out_cls_logits = model.module.cls_head(fpn_feats, fpn_masks)\n",
    "# out_offset: List[B, 2, T_i]\n",
    "out_offsets = model.module.reg_head(fpn_feats, fpn_masks)\n",
    "print(\"out_cls_logits: \" + str([tens.shape for tens in out_cls_logits]))\n",
    "print(\"out_offsets: \" + str([tens.shape for tens in out_offsets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute the outputs\n",
    "# out_cls: F List[B, #cls, T_i] -> F List[B, T_i, #cls]\n",
    "out_cls_logits = [x.permute(0, 2, 1) for x in out_cls_logits]\n",
    "# out_offset: F List[B, 2 (xC), T_i] -> F List[B, T_i, 2 (xC)]\n",
    "out_offsets = [x.permute(0, 2, 1) for x in out_offsets]\n",
    "# fpn_masks: F list[B, 1, T_i] -> F List[B, T_i]\n",
    "fpn_masks = [x.squeeze(1) for x in fpn_masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'video_id': '7729_102255', 'segments': tensor([[6.3763e+00, 1.3796e+01],\n",
      "        [6.3764e+00, 1.3796e+01],\n",
      "        [6.3763e+00, 1.3796e+01],\n",
      "        [1.1267e+01, 1.3854e+01],\n",
      "        [1.3300e+01, 1.3830e+01],\n",
      "        [3.2188e+01, 3.3565e+01],\n",
      "        [1.2663e+01, 1.3841e+01],\n",
      "        [6.3468e+00, 6.6661e+00],\n",
      "        [3.3486e+01, 3.3565e+01],\n",
      "        [-0.0000e+00, 2.1852e-02],\n",
      "        [8.1597e+00, 8.7350e+00],\n",
      "        [2.4670e+01, 2.8358e+01],\n",
      "        [3.9993e+00, 4.2418e+00],\n",
      "        [2.0589e+01, 2.4126e+01],\n",
      "        [6.9641e+00, 7.4834e+00],\n",
      "        [2.1886e+01, 2.2124e+01],\n",
      "        [-0.0000e+00, 1.8608e-01],\n",
      "        [8.7443e+00, 9.3431e+00],\n",
      "        [1.4058e+01, 1.4749e+01],\n",
      "        [2.3765e+01, 2.3997e+01],\n",
      "        [2.7434e+01, 2.7670e+01],\n",
      "        [2.2618e+00, 2.4992e+00],\n",
      "        [3.1587e+01, 3.1823e+01],\n",
      "        [2.5993e+01, 2.6226e+01],\n",
      "        [3.0887e+01, 3.1125e+01],\n",
      "        [2.5648e+01, 2.5880e+01],\n",
      "        [5.4593e+00, 5.6930e+00],\n",
      "        [1.9571e+01, 1.9807e+01],\n",
      "        [1.9265e+01, 1.9498e+01],\n",
      "        [6.0718e+00, 6.3116e+00],\n",
      "        [3.6203e+00, 3.8542e+00],\n",
      "        [1.6558e+01, 1.6788e+01],\n",
      "        [1.7606e+01, 1.7838e+01],\n",
      "        [1.6209e+01, 1.6441e+01],\n",
      "        [4.6246e+00, 4.8590e+00],\n",
      "        [1.0844e+00, 1.3208e+00],\n",
      "        [7.6332e+00, 8.1782e+00],\n",
      "        [2.7047e+01, 2.7282e+01],\n",
      "        [1.5685e+01, 1.5917e+01],\n",
      "        [2.3979e+01, 2.4984e+01],\n",
      "        [2.6654e+01, 2.6887e+01],\n",
      "        [1.5073e+01, 1.5300e+01],\n",
      "        [1.5988e+01, 1.6223e+01],\n",
      "        [3.0950e+00, 3.3288e+00],\n",
      "        [5.1515e+00, 5.3835e+00],\n",
      "        [1.1425e+01, 1.1867e+01],\n",
      "        [2.1497e+01, 2.1726e+01],\n",
      "        [3.0676e+01, 3.0910e+01],\n",
      "        [3.1284e+01, 3.1518e+01],\n",
      "        [1.7871e+01, 1.8103e+01],\n",
      "        [1.9173e+00, 2.1533e+00],\n",
      "        [3.3895e-01, 5.7553e-01],\n",
      "        [1.5425e+01, 1.5652e+01],\n",
      "        [1.4858e+01, 1.5088e+01],\n",
      "        [3.0283e+01, 3.0514e+01],\n",
      "        [2.9496e+01, 2.9729e+01],\n",
      "        [2.8227e+01, 2.8462e+01],\n",
      "        [2.9801e+01, 3.0035e+01],\n",
      "        [1.8874e+01, 1.9104e+01],\n",
      "        [2.1233e+01, 2.1468e+01],\n",
      "        [5.7663e+00, 6.0023e+00],\n",
      "        [1.8306e+01, 1.8541e+01],\n",
      "        [4.4082e+00, 4.6393e+00],\n",
      "        [2.7879e+01, 2.8112e+01],\n",
      "        [2.8664e+01, 2.8900e+01],\n",
      "        [2.3418e+01, 2.3646e+01],\n",
      "        [1.9968e+01, 2.0203e+01],\n",
      "        [1.8613e+01, 1.8846e+01],\n",
      "        [2.0928e+01, 2.1163e+01],\n",
      "        [1.7389e+01, 1.7619e+01],\n",
      "        [2.5166e+01, 2.5392e+01],\n",
      "        [4.8452e+00, 5.0786e+00],\n",
      "        [9.3461e+00, 9.9531e+00],\n",
      "        [1.0418e+01, 1.0882e+01],\n",
      "        [7.3654e-01, 9.7259e-01],\n",
      "        [1.7085e+01, 1.7316e+01],\n",
      "        [2.6610e+00, 2.8947e+00],\n",
      "        [2.2329e+01, 2.2563e+01],\n",
      "        [1.6545e+00, 1.8894e+00],\n",
      "        [3.1854e+01, 3.2090e+01],\n",
      "        [2.0580e+01, 2.0814e+01],\n",
      "        [3.2422e+01, 3.2660e+01],\n",
      "        [2.2984e+01, 2.3217e+01],\n",
      "        [2.9232e+01, 2.9464e+01],\n",
      "        [2.2720e+01, 2.2951e+01],\n",
      "        [3.3079e+01, 3.3314e+01],\n",
      "        [1.0902e+01, 1.1359e+01],\n",
      "        [2.6392e+01, 2.6624e+01],\n",
      "        [2.8927e+01, 2.9161e+01],\n",
      "        [1.3937e+00, 1.6285e+00],\n",
      "        [2.3202e+01, 2.3435e+01],\n",
      "        [3.2816e+01, 3.3052e+01],\n",
      "        [3.0020e+01, 3.0253e+01],\n",
      "        [2.0362e+01, 2.0596e+01],\n",
      "        [-0.0000e+00, 2.1852e-02],\n",
      "        [3.2118e+01, 3.2354e+01],\n",
      "        [3.3145e+00, 3.5404e+00],\n",
      "        [1.6821e+01, 1.7052e+01],\n",
      "        [2.4951e+01, 2.5184e+01],\n",
      "        [2.8446e+01, 2.8681e+01]]), 'scores': tensor([0.9677, 0.2583, 0.0666, 0.0112, 0.0088, 0.0067, 0.0063, 0.0063, 0.0061,\n",
      "        0.0058, 0.0057, 0.0057, 0.0056, 0.0055, 0.0055, 0.0054, 0.0054, 0.0053,\n",
      "        0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0052,\n",
      "        0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052,\n",
      "        0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0051, 0.0051, 0.0051, 0.0051,\n",
      "        0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051,\n",
      "        0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051,\n",
      "        0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0050,\n",
      "        0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "        0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "        0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
      "        0.0050]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])}]\n"
     ]
    }
   ],
   "source": [
    "results = model.module.inference(\n",
    "            sample, points, fpn_masks,\n",
    "            out_cls_logits, out_offsets\n",
    "        )\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]['segments'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test'] subset has 508 videos\n",
      "torch.Size([4096, 768])\n",
      "=> loading checkpoint 'Pretrained/tvil_tsn/epoch_015.pth.tar'\n",
      "Loading from EMA model ...\n",
      "[{'video_id': 'bd43315417_result', 'segments': tensor([[5.5437e+00, 5.7964e+00],\n",
      "        [4.0415e+00, 4.2007e+00],\n",
      "        [4.0436e+00, 4.1991e+00],\n",
      "        [4.3469e-01, 7.0177e-01],\n",
      "        [5.5437e+00, 5.7963e+00],\n",
      "        [3.1893e+00, 3.6245e+00],\n",
      "        [3.6801e+00, 4.2201e+00],\n",
      "        [4.5503e+00, 4.8542e+00],\n",
      "        [2.7800e-01, 7.0690e-01],\n",
      "        [4.2425e+00, 4.5209e+00],\n",
      "        [4.9046e+00, 5.2331e+00],\n",
      "        [2.1534e+00, 2.6493e+00],\n",
      "        [4.5322e+00, 4.6894e+00],\n",
      "        [4.0391e+00, 4.2011e+00],\n",
      "        [5.4987e+00, 5.8991e+00],\n",
      "        [3.6792e+00, 3.9432e+00],\n",
      "        [1.2801e+00, 1.7348e+00],\n",
      "        [5.8361e+00, 6.0000e+00],\n",
      "        [4.1146e+00, 4.6074e+00],\n",
      "        [5.1962e+00, 5.5005e+00],\n",
      "        [2.7198e+00, 3.2056e+00],\n",
      "        [7.6770e-01, 1.0269e+00],\n",
      "        [4.6862e-01, 7.0170e-01],\n",
      "        [3.1852e+00, 3.6229e+00],\n",
      "        [5.5974e+00, 5.7946e+00],\n",
      "        [3.7703e+00, 4.2496e+00],\n",
      "        [2.3631e+00, 2.6346e+00],\n",
      "        [4.5770e+00, 5.0193e+00],\n",
      "        [5.3817e+00, 5.5102e+00],\n",
      "        [1.9639e+00, 2.2837e+00],\n",
      "        [2.2166e-01, 7.1154e-01],\n",
      "        [1.4724e+00, 1.7217e+00],\n",
      "        [4.9943e+00, 5.5846e+00],\n",
      "        [6.8068e-01, 1.1700e+00],\n",
      "        [3.4732e+00, 3.6237e+00],\n",
      "        [4.2477e+00, 4.4112e+00],\n",
      "        [1.9967e+00, 2.5991e+00],\n",
      "        [9.2365e-04, 3.7488e-01],\n",
      "        [2.7072e+00, 2.9962e+00],\n",
      "        [2.4880e+00, 2.6291e+00],\n",
      "        [4.9042e+00, 5.2339e+00],\n",
      "        [4.5282e+00, 4.6598e+00],\n",
      "        [5.6581e+00, 5.7930e+00],\n",
      "        [3.6415e+00, 3.8867e+00],\n",
      "        [1.6358e+00, 2.1361e+00],\n",
      "        [4.5999e+00, 4.8701e+00],\n",
      "        [5.8966e+00, 5.9979e+00],\n",
      "        [1.5729e+00, 1.7081e+00],\n",
      "        [1.0812e+00, 1.4390e+00],\n",
      "        [1.2646e-03, 9.8334e-02],\n",
      "        [4.0485e+00, 4.4152e+00],\n",
      "        [5.2954e+00, 5.4702e+00],\n",
      "        [6.5162e-01, 8.9998e-01],\n",
      "        [2.7808e+00, 3.3366e+00],\n",
      "        [5.3981e+00, 5.6252e+00],\n",
      "        [2.5834e+00, 2.8068e+00],\n",
      "        [3.2990e+00, 3.7594e+00],\n",
      "        [5.1084e+00, 5.2379e+00],\n",
      "        [5.5512e+00, 5.6827e+00],\n",
      "        [3.8980e+00, 4.2059e+00],\n",
      "        [3.1773e+00, 3.3413e+00],\n",
      "        [5.8058e+00, 6.0000e+00],\n",
      "        [4.3673e+00, 4.5134e+00],\n",
      "        [9.6081e-01, 1.5825e+00],\n",
      "        [1.6054e+00, 1.8872e+00],\n",
      "        [8.0039e-01, 9.7387e-01],\n",
      "        [4.9934e-01, 7.2090e-01],\n",
      "        [2.8721e+00, 3.1555e+00],\n",
      "        [3.4944e+00, 3.7051e+00],\n",
      "        [4.9064e+00, 5.0737e+00],\n",
      "        [2.6729e+00, 2.9163e+00],\n",
      "        [-0.0000e+00, 2.6641e-01],\n",
      "        [2.2968e+00, 2.6764e+00],\n",
      "        [5.2718e+00, 6.0000e+00],\n",
      "        [3.6818e+00, 3.8181e+00],\n",
      "        [4.4850e+00, 4.9248e+00],\n",
      "        [5.9844e+00, 6.0000e+00],\n",
      "        [5.7456e+00, 5.8755e+00],\n",
      "        [2.1789e+00, 2.3275e+00],\n",
      "        [1.7338e+00, 2.0066e+00],\n",
      "        [4.2102e+00, 4.6111e+00],\n",
      "        [4.0804e+00, 4.2107e+00],\n",
      "        [-0.0000e+00, 2.3604e-02],\n",
      "        [9.6276e-02, 6.5644e-01],\n",
      "        [1.3161e+00, 1.7055e+00],\n",
      "        [3.7184e+00, 4.1004e+00],\n",
      "        [5.3392e+00, 5.4909e+00],\n",
      "        [5.9253e+00, 6.0000e+00],\n",
      "        [1.0416e+00, 1.0795e+00],\n",
      "        [3.1510e+00, 3.1889e+00],\n",
      "        [1.1588e+00, 1.1967e+00],\n",
      "        [1.2136e+00, 1.2514e+00],\n",
      "        [4.8696e+00, 4.9076e+00],\n",
      "        [1.2527e+00, 1.2905e+00],\n",
      "        [4.0110e-01, 4.3882e-01],\n",
      "        [1.0026e+00, 1.0404e+00],\n",
      "        [2.1277e+00, 2.1655e+00],\n",
      "        [3.6205e-01, 3.9981e-01],\n",
      "        [1.4400e+00, 1.4779e+00],\n",
      "        [2.3230e+00, 2.3608e+00]]), 'scores': tensor([0.6316, 0.6204, 0.1522, 0.1463, 0.1427, 0.1063, 0.1013, 0.0910, 0.0758,\n",
      "        0.0603, 0.0592, 0.0556, 0.0509, 0.0452, 0.0404, 0.0386, 0.0362, 0.0357,\n",
      "        0.0353, 0.0342, 0.0339, 0.0311, 0.0300, 0.0289, 0.0287, 0.0281, 0.0257,\n",
      "        0.0256, 0.0249, 0.0247, 0.0199, 0.0197, 0.0193, 0.0185, 0.0172, 0.0167,\n",
      "        0.0160, 0.0156, 0.0149, 0.0145, 0.0141, 0.0134, 0.0134, 0.0129, 0.0128,\n",
      "        0.0128, 0.0126, 0.0124, 0.0122, 0.0121, 0.0118, 0.0116, 0.0111, 0.0102,\n",
      "        0.0100, 0.0098, 0.0094, 0.0094, 0.0089, 0.0083, 0.0079, 0.0077, 0.0077,\n",
      "        0.0077, 0.0075, 0.0075, 0.0072, 0.0070, 0.0069, 0.0066, 0.0062, 0.0062,\n",
      "        0.0061, 0.0060, 0.0060, 0.0058, 0.0057, 0.0057, 0.0057, 0.0057, 0.0056,\n",
      "        0.0055, 0.0054, 0.0054, 0.0053, 0.0053, 0.0051, 0.0050, 0.0048, 0.0048,\n",
      "        0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0048,\n",
      "        0.0048]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])}]\n"
     ]
    }
   ],
   "source": [
    "# apply forward with TVIL data\n",
    "# model to load\n",
    "# video with two inpaintings: bd43315417_result\n",
    "config='configs/UMMAFormer/tvil_tsn.yaml'\n",
    "ckpt='Pretrained/tvil_tsn/epoch_015.pth.tar'\n",
    "epoch=15\n",
    "_ = fix_random_seed(0, include_cuda=True)\n",
    "# load config\n",
    "if os.path.isfile(config):\n",
    "    cfg = load_config(config)\n",
    "# data\n",
    "val_dataset = make_dataset(\n",
    "    cfg['dataset_name'], False, cfg['test_split'], **cfg['dataset']\n",
    ")\n",
    "# set bs = 1, and disable shuffle\n",
    "val_loader = make_data_loader(\n",
    "    val_dataset, False, None, 1, cfg['loader']['num_workers']\n",
    ")\n",
    "# Get a specific sample from val_dataset given the video_id\n",
    "sample = tsf.get_sample_by_video_id(val_dataset, 'bd43315417_result')\n",
    "print(sample[0][\"feats\"].shape)\n",
    "# model\n",
    "model = make_meta_arch(cfg['model_name'], **cfg['model'])\n",
    "# only one gpu\n",
    "model = nn.DataParallel(model, device_ids=cfg['devices'])\n",
    "# load checkpoint\n",
    "print(\"=> loading checkpoint '{}'\".format(ckpt))\n",
    "# load ckpt, reset epoch / best rmse\n",
    "checkpoint = torch.load(\n",
    "    ckpt,\n",
    "    map_location = lambda storage, loc: storage.cuda(cfg['devices'][0])\n",
    ")\n",
    "# load ema model instead\n",
    "print(\"Loading from EMA model ...\")\n",
    "model.load_state_dict(checkpoint['state_dict_ema'])\n",
    "del checkpoint\n",
    "model.eval()\n",
    "# forward pass\n",
    "results = model.module.forward(sample)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"position: relative; width: 640px;\">\n",
       "        <video id=\"vid\" width=\"640\" controls style=\"display: block;\">\n",
       "            <source src=\"data/TVIL/videos/test/bd43315417_result.mp4\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        <div id=\"progressbar-container\" style=\"position: relative; width: 640px; height: 10px; background: #eee; margin-top: 2px;\">\n",
       "            \n",
       "        <div style=\"\n",
       "            position: absolute;\n",
       "            left: 92.39505767822266%;\n",
       "            width: 4.2108306884765625%;\n",
       "            top: 0;\n",
       "            bottom: 0;\n",
       "            background: red;\n",
       "            opacity: 0.5;\n",
       "            pointer-events: none;\n",
       "        \"></div>\n",
       "        \n",
       "        <div style=\"\n",
       "            position: absolute;\n",
       "            left: 67.35884857177734%;\n",
       "            width: 2.6528167724609375%;\n",
       "            top: 0;\n",
       "            bottom: 0;\n",
       "            background: red;\n",
       "            opacity: 0.5;\n",
       "            pointer-events: none;\n",
       "        \"></div>\n",
       "        \n",
       "            <div id=\"progressbar-current\" style=\"position: absolute; left: 0; top: 0; bottom: 0; width: 0%; background: #2196F3; opacity: 0.8;\"></div>\n",
       "        </div>\n",
       "    </div>\n",
       "    <script>\n",
       "    (function() {\n",
       "        var video = document.getElementById('vid');\n",
       "        var progress = document.getElementById('progressbar-current');\n",
       "        var container = document.getElementById('progressbar-container');\n",
       "        video.addEventListener('timeupdate', function() {\n",
       "            var percent = 100 * video.currentTime / video.duration;\n",
       "            progress.style.width = percent + '%';\n",
       "        });\n",
       "        // Allow clicking on the progress bar to seek\n",
       "        container.addEventListener('click', function(e) {\n",
       "            var rect = container.getBoundingClientRect();\n",
       "            var x = e.clientX - rect.left;\n",
       "            var percent = x / rect.width;\n",
       "            video.currentTime = percent * video.duration;\n",
       "        });\n",
       "    })();\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the sample and results\n",
    "video_id = results[0]['video_id']\n",
    "segments = results[0]['segments'].cpu().numpy()\n",
    "scores = results[0]['scores'].cpu().numpy()\n",
    "tsf.show_vid_with_segments(video_id=video_id, segments=segments, scores=scores, cfg=cfg, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'audio_feat_folder': './data/Psynd/feats/byola',\n",
      "             'audio_file_ext': None,\n",
      "             'audio_input_dim': 2048,\n",
      "             'crop_ratio': [0.9, 1.0],\n",
      "             'default_fps': None,\n",
      "             'downsample_rate': 1,\n",
      "             'feat_stride': 1,\n",
      "             'file_ext': '.npy',\n",
      "             'file_prefix': 'none',\n",
      "             'force_upsampling': True,\n",
      "             'input_dim': 0,\n",
      "             'json_file': './data/Psynd/annotations/metadatabyola.json',\n",
      "             'max_seq_len': 768,\n",
      "             'num_classes': 1,\n",
      "             'num_frames': 1,\n",
      "             'trunc_thresh': 0.5},\n",
      " 'dataset_name': 'psynd',\n",
      " 'devices': [0],\n",
      " 'init_rand_seed': 1234567891,\n",
      " 'loader': {'batch_size': 8, 'num_workers': 4},\n",
      " 'model': {'audio_input_dim': 2048,\n",
      "           'backbone_arch': (2, 2, 5),\n",
      "           'backbone_type': 'convHRLRFullResSelfAttTransformerRevised',\n",
      "           'embd_dim': 256,\n",
      "           'embd_kernel_size': 3,\n",
      "           'embd_with_ln': True,\n",
      "           'fpn_dim': 256,\n",
      "           'fpn_start_level': 0,\n",
      "           'fpn_type': 'fpn',\n",
      "           'fpn_with_ln': True,\n",
      "           'head_dim': 256,\n",
      "           'head_kernel_size': 3,\n",
      "           'head_num_layers': 3,\n",
      "           'head_with_ln': True,\n",
      "           'input_dim': 0,\n",
      "           'max_buffer_len_factor': 1.0,\n",
      "           'max_seq_len': 768,\n",
      "           'n_head': 4,\n",
      "           'n_mha_win_size': [7, 7, 7, 7, 7, -1],\n",
      "           'num_classes': 1,\n",
      "           'regression_range': [(0, 4),\n",
      "                                (4, 8),\n",
      "                                (8, 16),\n",
      "                                (16, 32),\n",
      "                                (32, 64),\n",
      "                                (64, 10000)],\n",
      "           'scale_factor': 2,\n",
      "           'test_cfg': {'duration_thresh': 0.001,\n",
      "                        'ext_score_file': None,\n",
      "                        'iou_threshold': 0.1,\n",
      "                        'max_seg_num': 100,\n",
      "                        'min_score': 0.001,\n",
      "                        'multiclass_nms': False,\n",
      "                        'nms_method': 'soft',\n",
      "                        'nms_sigma': 0.75,\n",
      "                        'pre_nms_thresh': 0.001,\n",
      "                        'pre_nms_topk': 2000,\n",
      "                        'voting_thresh': 0.9},\n",
      "           'train_cfg': {'center_sample': 'radius',\n",
      "                         'center_sample_radius': 1.5,\n",
      "                         'clip_grad_l2norm': 1.0,\n",
      "                         'cls_prior_prob': 0.01,\n",
      "                         'dropout': 0.0,\n",
      "                         'droppath': 0.1,\n",
      "                         'head_empty_cls': [],\n",
      "                         'init_loss_norm': 200,\n",
      "                         'label_smoothing': 0.1,\n",
      "                         'loss_weight': 2.0},\n",
      "           'use_abs_pe': True,\n",
      "           'use_rel_pe': False},\n",
      " 'model_name': 'AVLocPointTransformerRecoveryNoNorm',\n",
      " 'opt': {'epochs': 10,\n",
      "         'learning_rate': 0.001,\n",
      "         'momentum': 0.9,\n",
      "         'schedule_gamma': 0.1,\n",
      "         'schedule_steps': [],\n",
      "         'schedule_type': 'cosine',\n",
      "         'type': 'AdamW',\n",
      "         'warmup': True,\n",
      "         'warmup_epochs': 5,\n",
      "         'weight_decay': 0.05},\n",
      " 'output_folder': './paper_results/Revised',\n",
      " 'test_cfg': {'duration_thresh': 0.001,\n",
      "              'ext_score_file': None,\n",
      "              'iou_threshold': 0.1,\n",
      "              'max_seg_num': 100,\n",
      "              'min_score': 0.001,\n",
      "              'multiclass_nms': False,\n",
      "              'nms_method': 'soft',\n",
      "              'nms_sigma': 0.75,\n",
      "              'pre_nms_thresh': 0.001,\n",
      "              'pre_nms_topk': 2000,\n",
      "              'voting_thresh': 0.9},\n",
      " 'test_split': ['test'],\n",
      " 'train_cfg': {'center_sample': 'radius',\n",
      "               'center_sample_radius': 1.5,\n",
      "               'clip_grad_l2norm': 1.0,\n",
      "               'cls_prior_prob': 0.01,\n",
      "               'dropout': 0.0,\n",
      "               'droppath': 0.1,\n",
      "               'head_empty_cls': [],\n",
      "               'init_loss_norm': 200,\n",
      "               'label_smoothing': 0.1,\n",
      "               'loss_weight': 2.0},\n",
      " 'train_split': ['train'],\n",
      " 'val_split': ['dev']}\n",
      "['train'] subset has 2263 videos\n",
      "['dev'] subset has 108 videos\n",
      "Using model EMA ...\n",
      "\n",
      "Start training model AVLocPointTransformerRecoveryNoNorm ...\n",
      "\n",
      "[Train]: Epoch 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/Thesis/UMMAFormerTest/venv/lib/python3.10/site-packages/torch/autograd/graph.py:824: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:93.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [000][00010/00282]\tTime 1.16 (1.16)\tLoss 11.25 (11.25)\n",
      "\t\tcls_loss 0.38 (0.38)\treg_loss 0.26 (0.26)\treco_loss 10.31 (10.31)\treco_cls_loss 0.41 (0.41)\n",
      "Epoch: [000][00020/00282]\tTime 0.64 (0.90)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.74 (0.56)\treg_loss 0.53 (0.39)\treco_loss nan (nan)\treco_cls_loss 0.30 (0.36)\n",
      "Epoch: [000][00030/00282]\tTime 0.63 (0.81)\tLoss 12.79 (nan)\n",
      "\t\tcls_loss 0.68 (0.60)\treg_loss 0.56 (0.45)\treco_loss 10.95 (nan)\treco_cls_loss 0.45 (0.39)\n",
      "Epoch: [000][00040/00282]\tTime 0.64 (0.76)\tLoss 13.85 (nan)\n",
      "\t\tcls_loss 0.56 (0.59)\treg_loss 0.30 (0.41)\treco_loss 12.64 (nan)\treco_cls_loss 0.48 (0.41)\n",
      "Epoch: [000][00050/00282]\tTime 0.63 (0.74)\tLoss 14.08 (nan)\n",
      "\t\tcls_loss 0.73 (0.62)\treg_loss 0.45 (0.42)\treco_loss 12.40 (nan)\treco_cls_loss 0.53 (0.44)\n",
      "Epoch: [000][00060/00282]\tTime 0.63 (0.72)\tLoss 12.84 (nan)\n",
      "\t\tcls_loss 0.42 (0.58)\treg_loss 0.32 (0.40)\treco_loss 11.71 (nan)\treco_cls_loss 0.74 (0.49)\n",
      "Epoch: [000][00070/00282]\tTime 0.64 (0.71)\tLoss 12.52 (nan)\n",
      "\t\tcls_loss 0.58 (0.58)\treg_loss 0.37 (0.40)\treco_loss 11.16 (nan)\treco_cls_loss 0.50 (0.49)\n",
      "Epoch: [000][00080/00282]\tTime 0.62 (0.70)\tLoss 12.14 (nan)\n",
      "\t\tcls_loss 0.61 (0.59)\treg_loss 0.37 (0.39)\treco_loss 10.76 (nan)\treco_cls_loss 0.37 (0.47)\n",
      "Epoch: [000][00090/00282]\tTime 0.63 (0.69)\tLoss 12.87 (nan)\n",
      "\t\tcls_loss 0.42 (0.57)\treg_loss 0.23 (0.38)\treco_loss 11.94 (nan)\treco_cls_loss 0.61 (0.49)\n",
      "Epoch: [000][00100/00282]\tTime 0.65 (0.69)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.60 (0.57)\treg_loss 0.39 (0.38)\treco_loss nan (nan)\treco_cls_loss 0.11 (0.45)\n",
      "Epoch: [000][00110/00282]\tTime 0.64 (0.68)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.52 (0.57)\treg_loss 0.29 (0.37)\treco_loss nan (nan)\treco_cls_loss 0.09 (0.42)\n",
      "Epoch: [000][00120/00282]\tTime 0.66 (0.68)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.56 (0.57)\treg_loss 0.28 (0.36)\treco_loss nan (nan)\treco_cls_loss 0.12 (0.39)\n",
      "Epoch: [000][00130/00282]\tTime 0.63 (0.68)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.54 (0.56)\treg_loss 0.23 (0.35)\treco_loss nan (nan)\treco_cls_loss 0.23 (0.38)\n",
      "Epoch: [000][00140/00282]\tTime 0.64 (0.67)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.65 (0.57)\treg_loss 0.21 (0.34)\treco_loss nan (nan)\treco_cls_loss 0.15 (0.36)\n",
      "Epoch: [000][00150/00282]\tTime 0.63 (0.67)\tLoss 12.39 (nan)\n",
      "\t\tcls_loss 0.48 (0.56)\treg_loss 0.15 (0.33)\treco_loss 11.58 (nan)\treco_cls_loss 0.27 (0.36)\n",
      "Epoch: [000][00160/00282]\tTime 0.63 (0.67)\tLoss 12.56 (nan)\n",
      "\t\tcls_loss 0.53 (0.56)\treg_loss 0.15 (0.32)\treco_loss 11.70 (nan)\treco_cls_loss 0.41 (0.36)\n",
      "Epoch: [000][00170/00282]\tTime 0.62 (0.66)\tLoss 11.71 (nan)\n",
      "\t\tcls_loss 0.19 (0.54)\treg_loss 0.04 (0.30)\treco_loss 11.34 (nan)\treco_cls_loss 0.89 (0.39)\n",
      "Epoch: [000][00180/00282]\tTime 0.62 (0.66)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.52 (0.54)\treg_loss 0.18 (0.29)\treco_loss nan (nan)\treco_cls_loss 0.27 (0.39)\n",
      "Epoch: [000][00190/00282]\tTime 0.62 (0.66)\tLoss 13.58 (nan)\n",
      "\t\tcls_loss 0.44 (0.53)\treg_loss 0.16 (0.29)\treco_loss 12.81 (nan)\treco_cls_loss 0.12 (0.37)\n",
      "Epoch: [000][00200/00282]\tTime 0.62 (0.66)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.68 (0.54)\treg_loss 0.21 (0.28)\treco_loss nan (nan)\treco_cls_loss 0.12 (0.36)\n",
      "Epoch: [000][00210/00282]\tTime 0.63 (0.66)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.64 (0.55)\treg_loss 0.21 (0.28)\treco_loss nan (nan)\treco_cls_loss 0.26 (0.35)\n",
      "Epoch: [000][00220/00282]\tTime 0.63 (0.66)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.47 (0.54)\treg_loss 0.19 (0.28)\treco_loss nan (nan)\treco_cls_loss 0.09 (0.34)\n",
      "Epoch: [000][00230/00282]\tTime 0.62 (0.65)\tLoss 11.77 (nan)\n",
      "\t\tcls_loss 0.34 (0.53)\treg_loss 0.07 (0.27)\treco_loss 11.26 (nan)\treco_cls_loss 0.21 (0.34)\n",
      "Epoch: [000][00240/00282]\tTime 0.63 (0.65)\tLoss 10.49 (nan)\n",
      "\t\tcls_loss 0.48 (0.53)\treg_loss 0.11 (0.26)\treco_loss 9.77 (nan)\treco_cls_loss 0.23 (0.33)\n",
      "Epoch: [000][00250/00282]\tTime 0.64 (0.65)\tLoss 11.24 (nan)\n",
      "\t\tcls_loss 0.32 (0.52)\treg_loss 0.08 (0.25)\treco_loss 10.77 (nan)\treco_cls_loss 0.08 (0.32)\n",
      "Epoch: [000][00260/00282]\tTime 0.64 (0.65)\tLoss 12.75 (nan)\n",
      "\t\tcls_loss 0.41 (0.52)\treg_loss 0.07 (0.25)\treco_loss 12.19 (nan)\treco_cls_loss 0.12 (0.31)\n",
      "Epoch: [000][00270/00282]\tTime 0.62 (0.65)\tLoss 12.16 (nan)\n",
      "\t\tcls_loss 0.52 (0.52)\treg_loss 0.16 (0.24)\treco_loss 11.30 (nan)\treco_cls_loss 0.19 (0.31)\n",
      "Epoch: [000][00280/00282]\tTime 0.62 (0.65)\tLoss 12.04 (nan)\n",
      "\t\tcls_loss 0.37 (0.51)\treg_loss 0.12 (0.24)\treco_loss 11.42 (nan)\treco_cls_loss 0.04 (0.30)\n",
      "[Train]: Epoch 0 finished with lr=0.00020014\n",
      "\n",
      "\n",
      "[Train]: Epoch 1 started\n",
      "Epoch: [001][00010/00282]\tTime 0.71 (0.71)\tLoss 12.50 (12.50)\n",
      "\t\tcls_loss 0.31 (0.31)\treg_loss 0.14 (0.14)\treco_loss 11.87 (11.87)\treco_cls_loss 0.34 (0.34)\n",
      "Epoch: [001][00020/00282]\tTime 0.63 (0.67)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.41 (0.36)\treg_loss 0.11 (0.13)\treco_loss nan (nan)\treco_cls_loss 0.02 (0.18)\n",
      "Epoch: [001][00030/00282]\tTime 0.62 (0.65)\tLoss 12.13 (nan)\n",
      "\t\tcls_loss 0.35 (0.35)\treg_loss 0.07 (0.11)\treco_loss 11.63 (nan)\treco_cls_loss 0.08 (0.15)\n",
      "Epoch: [001][00040/00282]\tTime 0.60 (0.64)\tLoss 12.78 (nan)\n",
      "\t\tcls_loss 0.37 (0.36)\treg_loss 0.08 (0.10)\treco_loss 12.24 (nan)\treco_cls_loss 0.07 (0.13)\n",
      "Epoch: [001][00050/00282]\tTime 0.60 (0.63)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.40 (0.37)\treg_loss 0.09 (0.10)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.11)\n",
      "Epoch: [001][00060/00282]\tTime 0.60 (0.63)\tLoss 11.80 (nan)\n",
      "\t\tcls_loss 0.34 (0.36)\treg_loss 0.09 (0.10)\treco_loss 11.29 (nan)\treco_cls_loss 0.07 (0.10)\n",
      "Epoch: [001][00070/00282]\tTime 0.61 (0.62)\tLoss 13.40 (nan)\n",
      "\t\tcls_loss 0.41 (0.37)\treg_loss 0.17 (0.11)\treco_loss 12.65 (nan)\treco_cls_loss 0.05 (0.09)\n",
      "Epoch: [001][00080/00282]\tTime 0.61 (0.62)\tLoss 11.93 (nan)\n",
      "\t\tcls_loss 0.33 (0.36)\treg_loss 0.11 (0.11)\treco_loss 11.38 (nan)\treco_cls_loss 0.05 (0.09)\n",
      "Epoch: [001][00090/00282]\tTime 0.61 (0.62)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.47 (0.38)\treg_loss 0.09 (0.11)\treco_loss nan (nan)\treco_cls_loss 0.07 (0.09)\n",
      "Epoch: [001][00100/00282]\tTime 0.60 (0.62)\tLoss 11.45 (nan)\n",
      "\t\tcls_loss 0.36 (0.37)\treg_loss 0.06 (0.10)\treco_loss 10.96 (nan)\treco_cls_loss 0.03 (0.08)\n",
      "Epoch: [001][00110/00282]\tTime 0.61 (0.62)\tLoss 11.87 (nan)\n",
      "\t\tcls_loss 0.40 (0.38)\treg_loss 0.10 (0.10)\treco_loss 11.27 (nan)\treco_cls_loss 0.03 (0.08)\n",
      "Epoch: [001][00120/00282]\tTime 0.60 (0.62)\tLoss 11.53 (nan)\n",
      "\t\tcls_loss 0.30 (0.37)\treg_loss 0.09 (0.10)\treco_loss 11.04 (nan)\treco_cls_loss 0.05 (0.07)\n",
      "Epoch: [001][00130/00282]\tTime 0.62 (0.62)\tLoss 12.09 (nan)\n",
      "\t\tcls_loss 0.20 (0.36)\treg_loss 0.04 (0.10)\treco_loss 11.80 (nan)\treco_cls_loss 0.10 (0.08)\n",
      "Epoch: [001][00140/00282]\tTime 0.60 (0.62)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.38 (0.36)\treg_loss 0.08 (0.10)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.07)\n",
      "Epoch: [001][00150/00282]\tTime 0.60 (0.61)\tLoss 11.04 (nan)\n",
      "\t\tcls_loss 0.31 (0.36)\treg_loss 0.06 (0.09)\treco_loss 10.60 (nan)\treco_cls_loss 0.01 (0.07)\n",
      "Epoch: [001][00160/00282]\tTime 0.61 (0.61)\tLoss 12.33 (nan)\n",
      "\t\tcls_loss 0.32 (0.35)\treg_loss 0.06 (0.09)\treco_loss 11.88 (nan)\treco_cls_loss 0.00 (0.06)\n",
      "Epoch: [001][00170/00282]\tTime 0.61 (0.61)\tLoss 11.59 (nan)\n",
      "\t\tcls_loss 0.28 (0.35)\treg_loss 0.08 (0.09)\treco_loss 11.13 (nan)\treco_cls_loss 0.03 (0.06)\n",
      "Epoch: [001][00180/00282]\tTime 0.59 (0.61)\tLoss 11.80 (nan)\n",
      "\t\tcls_loss 0.41 (0.35)\treg_loss 0.07 (0.09)\treco_loss 11.25 (nan)\treco_cls_loss 0.00 (0.06)\n",
      "Epoch: [001][00190/00282]\tTime 0.59 (0.61)\tLoss 12.44 (nan)\n",
      "\t\tcls_loss 0.26 (0.35)\treg_loss 0.08 (0.09)\treco_loss 12.00 (nan)\treco_cls_loss 0.23 (0.07)\n",
      "Epoch: [001][00200/00282]\tTime 0.60 (0.61)\tLoss 13.36 (nan)\n",
      "\t\tcls_loss 0.32 (0.35)\treg_loss 0.05 (0.09)\treco_loss 12.93 (nan)\treco_cls_loss 0.02 (0.06)\n",
      "Epoch: [001][00210/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.39 (0.35)\treg_loss 0.10 (0.09)\treco_loss nan (nan)\treco_cls_loss 0.13 (0.07)\n",
      "Epoch: [001][00220/00282]\tTime 0.61 (0.61)\tLoss 11.59 (nan)\n",
      "\t\tcls_loss 0.34 (0.35)\treg_loss 0.08 (0.09)\treco_loss 11.09 (nan)\treco_cls_loss 0.06 (0.07)\n",
      "Epoch: [001][00230/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.39 (0.35)\treg_loss 0.08 (0.09)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.06)\n",
      "Epoch: [001][00240/00282]\tTime 0.60 (0.61)\tLoss 10.61 (nan)\n",
      "\t\tcls_loss 0.28 (0.35)\treg_loss 0.08 (0.09)\treco_loss 10.15 (nan)\treco_cls_loss 0.18 (0.07)\n",
      "Epoch: [001][00250/00282]\tTime 0.60 (0.61)\tLoss 11.99 (nan)\n",
      "\t\tcls_loss 0.28 (0.34)\treg_loss 0.06 (0.09)\treco_loss 11.57 (nan)\treco_cls_loss 0.19 (0.07)\n",
      "Epoch: [001][00260/00282]\tTime 0.60 (0.61)\tLoss 11.65 (nan)\n",
      "\t\tcls_loss 0.35 (0.35)\treg_loss 0.07 (0.09)\treco_loss 11.14 (nan)\treco_cls_loss 0.14 (0.08)\n",
      "Epoch: [001][00270/00282]\tTime 0.61 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.43 (0.35)\treg_loss 0.17 (0.09)\treco_loss nan (nan)\treco_cls_loss 0.17 (0.08)\n",
      "Epoch: [001][00280/00282]\tTime 0.58 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.61 (0.36)\treg_loss 0.19 (0.09)\treco_loss nan (nan)\treco_cls_loss 0.06 (0.08)\n",
      "[Train]: Epoch 1 finished with lr=0.00040028\n",
      "\n",
      "\n",
      "[Train]: Epoch 2 started\n",
      "Epoch: [002][00010/00282]\tTime 0.69 (0.69)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.37 (0.37)\treg_loss 0.07 (0.07)\treco_loss nan (nan)\treco_cls_loss 0.06 (0.06)\n",
      "Epoch: [002][00020/00282]\tTime 0.60 (0.64)\tLoss 12.04 (nan)\n",
      "\t\tcls_loss 0.33 (0.35)\treg_loss 0.13 (0.10)\treco_loss 11.44 (nan)\treco_cls_loss 0.08 (0.07)\n",
      "Epoch: [002][00030/00282]\tTime 0.60 (0.63)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.36 (0.35)\treg_loss 0.07 (0.09)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.05)\n",
      "Epoch: [002][00040/00282]\tTime 0.59 (0.62)\tLoss 13.48 (nan)\n",
      "\t\tcls_loss 0.39 (0.36)\treg_loss 0.10 (0.10)\treco_loss 12.88 (nan)\treco_cls_loss 0.03 (0.05)\n",
      "Epoch: [002][00050/00282]\tTime 0.61 (0.62)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.36 (0.36)\treg_loss 0.12 (0.10)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.04)\n",
      "Epoch: [002][00060/00282]\tTime 0.59 (0.61)\tLoss 12.31 (nan)\n",
      "\t\tcls_loss 0.34 (0.36)\treg_loss 0.06 (0.09)\treco_loss 11.84 (nan)\treco_cls_loss 0.07 (0.04)\n",
      "Epoch: [002][00070/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.36 (0.36)\treg_loss 0.06 (0.09)\treco_loss nan (nan)\treco_cls_loss 0.26 (0.07)\n",
      "Epoch: [002][00080/00282]\tTime 0.60 (0.61)\tLoss 12.05 (nan)\n",
      "\t\tcls_loss 0.35 (0.36)\treg_loss 0.08 (0.09)\treco_loss 11.55 (nan)\treco_cls_loss 0.01 (0.07)\n",
      "Epoch: [002][00090/00282]\tTime 0.60 (0.61)\tLoss 11.78 (nan)\n",
      "\t\tcls_loss 0.31 (0.35)\treg_loss 0.07 (0.09)\treco_loss 11.32 (nan)\treco_cls_loss 0.03 (0.06)\n",
      "Epoch: [002][00100/00282]\tTime 0.60 (0.61)\tLoss 11.91 (nan)\n",
      "\t\tcls_loss 0.19 (0.33)\treg_loss 0.05 (0.08)\treco_loss 11.56 (nan)\treco_cls_loss 0.69 (0.12)\n",
      "Epoch: [002][00110/00282]\tTime 0.60 (0.61)\tLoss 11.92 (nan)\n",
      "\t\tcls_loss 0.41 (0.34)\treg_loss 0.11 (0.09)\treco_loss 11.28 (nan)\treco_cls_loss 0.06 (0.12)\n",
      "Epoch: [002][00120/00282]\tTime 0.60 (0.61)\tLoss 12.39 (nan)\n",
      "\t\tcls_loss 0.29 (0.34)\treg_loss 0.06 (0.08)\treco_loss 11.96 (nan)\treco_cls_loss 0.16 (0.12)\n",
      "Epoch: [002][00130/00282]\tTime 0.59 (0.61)\tLoss 11.69 (nan)\n",
      "\t\tcls_loss 0.28 (0.33)\treg_loss 0.04 (0.08)\treco_loss 11.32 (nan)\treco_cls_loss 0.07 (0.12)\n",
      "Epoch: [002][00140/00282]\tTime 0.60 (0.60)\tLoss 11.70 (nan)\n",
      "\t\tcls_loss 0.28 (0.33)\treg_loss 0.06 (0.08)\treco_loss 11.31 (nan)\treco_cls_loss 0.05 (0.11)\n",
      "Epoch: [002][00150/00282]\tTime 0.60 (0.60)\tLoss 11.20 (nan)\n",
      "\t\tcls_loss 0.29 (0.33)\treg_loss 0.03 (0.08)\treco_loss 10.84 (nan)\treco_cls_loss 0.03 (0.11)\n",
      "Epoch: [002][00160/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.41 (0.33)\treg_loss 0.11 (0.08)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.10)\n",
      "Epoch: [002][00170/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.36 (0.33)\treg_loss 0.04 (0.08)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.10)\n",
      "Epoch: [002][00180/00282]\tTime 0.60 (0.60)\tLoss 11.99 (nan)\n",
      "\t\tcls_loss 0.21 (0.33)\treg_loss 0.02 (0.07)\treco_loss 11.73 (nan)\treco_cls_loss 0.02 (0.09)\n",
      "Epoch: [002][00190/00282]\tTime 0.61 (0.60)\tLoss 10.96 (nan)\n",
      "\t\tcls_loss 0.27 (0.32)\treg_loss 0.05 (0.07)\treco_loss 10.58 (nan)\treco_cls_loss 0.09 (0.09)\n",
      "Epoch: [002][00200/00282]\tTime 0.60 (0.60)\tLoss 11.43 (nan)\n",
      "\t\tcls_loss 0.31 (0.32)\treg_loss 0.05 (0.07)\treco_loss 11.01 (nan)\treco_cls_loss 0.01 (0.09)\n",
      "Epoch: [002][00210/00282]\tTime 0.60 (0.60)\tLoss 11.39 (nan)\n",
      "\t\tcls_loss 0.25 (0.32)\treg_loss 0.04 (0.07)\treco_loss 11.06 (nan)\treco_cls_loss 0.08 (0.09)\n",
      "Epoch: [002][00220/00282]\tTime 0.61 (0.60)\tLoss 11.57 (nan)\n",
      "\t\tcls_loss 0.23 (0.31)\treg_loss 0.03 (0.07)\treco_loss 11.29 (nan)\treco_cls_loss 0.01 (0.08)\n",
      "Epoch: [002][00230/00282]\tTime 0.60 (0.60)\tLoss 13.23 (nan)\n",
      "\t\tcls_loss 0.32 (0.32)\treg_loss 0.07 (0.07)\treco_loss 12.76 (nan)\treco_cls_loss 0.01 (0.08)\n",
      "Epoch: [002][00240/00282]\tTime 0.61 (0.60)\tLoss 11.91 (nan)\n",
      "\t\tcls_loss 0.33 (0.32)\treg_loss 0.05 (0.07)\treco_loss 11.47 (nan)\treco_cls_loss 0.01 (0.08)\n",
      "Epoch: [002][00250/00282]\tTime 0.60 (0.60)\tLoss 12.14 (nan)\n",
      "\t\tcls_loss 0.33 (0.32)\treg_loss 0.06 (0.07)\treco_loss 11.68 (nan)\treco_cls_loss 0.02 (0.08)\n",
      "Epoch: [002][00260/00282]\tTime 0.60 (0.60)\tLoss 12.32 (nan)\n",
      "\t\tcls_loss 0.33 (0.32)\treg_loss 0.08 (0.07)\treco_loss 11.82 (nan)\treco_cls_loss 0.00 (0.07)\n",
      "Epoch: [002][00270/00282]\tTime 0.61 (0.60)\tLoss 11.69 (nan)\n",
      "\t\tcls_loss 0.32 (0.32)\treg_loss 0.06 (0.07)\treco_loss 11.25 (nan)\treco_cls_loss 0.00 (0.07)\n",
      "Epoch: [002][00280/00282]\tTime 0.59 (0.60)\tLoss 12.07 (nan)\n",
      "\t\tcls_loss 0.31 (0.32)\treg_loss 0.11 (0.07)\treco_loss 11.55 (nan)\treco_cls_loss 0.01 (0.07)\n",
      "[Train]: Epoch 2 finished with lr=0.00060043\n",
      "\n",
      "\n",
      "[Train]: Epoch 3 started\n",
      "Epoch: [003][00010/00282]\tTime 0.69 (0.69)\tLoss 11.84 (11.84)\n",
      "\t\tcls_loss 0.26 (0.26)\treg_loss 0.10 (0.10)\treco_loss 11.38 (11.38)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [003][00020/00282]\tTime 0.61 (0.65)\tLoss 12.50 (12.17)\n",
      "\t\tcls_loss 0.29 (0.28)\treg_loss 0.05 (0.08)\treco_loss 12.09 (11.74)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [003][00030/00282]\tTime 0.60 (0.63)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.37 (0.31)\treg_loss 0.06 (0.07)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [003][00040/00282]\tTime 0.59 (0.62)\tLoss 11.66 (nan)\n",
      "\t\tcls_loss 0.28 (0.30)\treg_loss 0.03 (0.06)\treco_loss 11.31 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [003][00050/00282]\tTime 0.59 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.44 (0.33)\treg_loss 0.08 (0.07)\treco_loss nan (nan)\treco_cls_loss 0.07 (0.02)\n",
      "Epoch: [003][00060/00282]\tTime 0.60 (0.61)\tLoss 11.63 (nan)\n",
      "\t\tcls_loss 0.30 (0.32)\treg_loss 0.04 (0.06)\treco_loss 11.25 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [003][00070/00282]\tTime 0.60 (0.61)\tLoss 12.08 (nan)\n",
      "\t\tcls_loss 0.33 (0.33)\treg_loss 0.09 (0.07)\treco_loss 11.56 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [003][00080/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.44 (0.34)\treg_loss 0.10 (0.07)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [003][00090/00282]\tTime 0.60 (0.61)\tLoss 11.80 (nan)\n",
      "\t\tcls_loss 0.32 (0.34)\treg_loss 0.13 (0.08)\treco_loss 11.21 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [003][00100/00282]\tTime 0.59 (0.61)\tLoss 9.85 (nan)\n",
      "\t\tcls_loss 0.30 (0.33)\treg_loss 0.10 (0.08)\treco_loss 9.36 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [003][00110/00282]\tTime 0.61 (0.61)\tLoss 12.89 (nan)\n",
      "\t\tcls_loss 0.30 (0.33)\treg_loss 0.05 (0.08)\treco_loss 12.50 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [003][00120/00282]\tTime 0.60 (0.61)\tLoss 11.62 (nan)\n",
      "\t\tcls_loss 0.45 (0.34)\treg_loss 0.15 (0.08)\treco_loss 10.88 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [003][00130/00282]\tTime 0.60 (0.60)\tLoss 12.59 (nan)\n",
      "\t\tcls_loss 0.31 (0.34)\treg_loss 0.16 (0.09)\treco_loss 11.96 (nan)\treco_cls_loss 0.01 (0.01)\n",
      "Epoch: [003][00140/00282]\tTime 0.59 (0.60)\tLoss 10.95 (nan)\n",
      "\t\tcls_loss 0.27 (0.33)\treg_loss 0.09 (0.09)\treco_loss 10.51 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [003][00150/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.33)\treg_loss 0.07 (0.09)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [003][00160/00282]\tTime 0.61 (0.60)\tLoss 12.73 (nan)\n",
      "\t\tcls_loss 0.33 (0.33)\treg_loss 0.11 (0.09)\treco_loss 12.18 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [003][00170/00282]\tTime 0.60 (0.60)\tLoss 11.77 (nan)\n",
      "\t\tcls_loss 0.28 (0.33)\treg_loss 0.05 (0.09)\treco_loss 11.38 (nan)\treco_cls_loss 0.19 (0.02)\n",
      "Epoch: [003][00180/00282]\tTime 0.60 (0.60)\tLoss 12.04 (nan)\n",
      "\t\tcls_loss 0.31 (0.33)\treg_loss 0.06 (0.08)\treco_loss 11.60 (nan)\treco_cls_loss 0.00 (0.02)\n",
      "Epoch: [003][00190/00282]\tTime 0.61 (0.60)\tLoss 11.64 (nan)\n",
      "\t\tcls_loss 0.35 (0.33)\treg_loss 0.08 (0.08)\treco_loss 11.14 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [003][00200/00282]\tTime 0.60 (0.60)\tLoss 12.40 (nan)\n",
      "\t\tcls_loss 0.35 (0.33)\treg_loss 0.07 (0.08)\treco_loss 11.91 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [003][00210/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.39 (0.33)\treg_loss 0.08 (0.08)\treco_loss nan (nan)\treco_cls_loss 0.19 (0.02)\n",
      "Epoch: [003][00220/00282]\tTime 0.60 (0.60)\tLoss 10.70 (nan)\n",
      "\t\tcls_loss 0.37 (0.34)\treg_loss 0.10 (0.08)\treco_loss 10.12 (nan)\treco_cls_loss 0.01 (0.02)\n",
      "Epoch: [003][00230/00282]\tTime 0.59 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.34)\treg_loss 0.06 (0.08)\treco_loss nan (nan)\treco_cls_loss 0.20 (0.03)\n",
      "Epoch: [003][00240/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.46 (0.34)\treg_loss 0.10 (0.08)\treco_loss nan (nan)\treco_cls_loss 0.14 (0.03)\n",
      "Epoch: [003][00250/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.39 (0.34)\treg_loss 0.08 (0.08)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [003][00260/00282]\tTime 0.59 (0.60)\tLoss 11.01 (nan)\n",
      "\t\tcls_loss 0.30 (0.34)\treg_loss 0.07 (0.08)\treco_loss 10.53 (nan)\treco_cls_loss 0.37 (0.05)\n",
      "Epoch: [003][00270/00282]\tTime 0.60 (0.60)\tLoss 12.16 (nan)\n",
      "\t\tcls_loss 0.27 (0.34)\treg_loss 0.04 (0.08)\treco_loss 11.81 (nan)\treco_cls_loss 0.02 (0.04)\n",
      "Epoch: [003][00280/00282]\tTime 0.59 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.34)\treg_loss 0.10 (0.08)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.04)\n",
      "[Train]: Epoch 3 finished with lr=0.00080057\n",
      "\n",
      "\n",
      "[Train]: Epoch 4 started\n",
      "Epoch: [004][00010/00282]\tTime 0.68 (0.68)\tLoss 11.71 (11.71)\n",
      "\t\tcls_loss 0.29 (0.29)\treg_loss 0.06 (0.06)\treco_loss 11.29 (11.29)\treco_cls_loss 0.03 (0.03)\n",
      "Epoch: [004][00020/00282]\tTime 0.60 (0.64)\tLoss 11.66 (11.69)\n",
      "\t\tcls_loss 0.27 (0.28)\treg_loss 0.05 (0.06)\treco_loss 11.29 (11.29)\treco_cls_loss 0.06 (0.05)\n",
      "Epoch: [004][00030/00282]\tTime 0.60 (0.63)\tLoss 12.32 (11.90)\n",
      "\t\tcls_loss 0.26 (0.27)\treg_loss 0.04 (0.05)\treco_loss 11.98 (11.52)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [004][00040/00282]\tTime 0.60 (0.62)\tLoss 11.18 (11.72)\n",
      "\t\tcls_loss 0.31 (0.28)\treg_loss 0.07 (0.06)\treco_loss 10.64 (11.30)\treco_cls_loss 0.85 (0.24)\n",
      "Epoch: [004][00050/00282]\tTime 0.59 (0.62)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.29)\treg_loss 0.06 (0.06)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.19)\n",
      "Epoch: [004][00060/00282]\tTime 0.59 (0.61)\tLoss 11.73 (nan)\n",
      "\t\tcls_loss 0.33 (0.30)\treg_loss 0.06 (0.06)\treco_loss 11.24 (nan)\treco_cls_loss 0.46 (0.23)\n",
      "Epoch: [004][00070/00282]\tTime 0.60 (0.61)\tLoss 10.96 (nan)\n",
      "\t\tcls_loss 0.31 (0.30)\treg_loss 0.05 (0.06)\treco_loss 10.55 (nan)\treco_cls_loss 0.02 (0.20)\n",
      "Epoch: [004][00080/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.37 (0.31)\treg_loss 0.09 (0.06)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.18)\n",
      "Epoch: [004][00090/00282]\tTime 0.60 (0.61)\tLoss 12.17 (nan)\n",
      "\t\tcls_loss 0.40 (0.32)\treg_loss 0.13 (0.07)\treco_loss 11.51 (nan)\treco_cls_loss 0.02 (0.16)\n",
      "Epoch: [004][00100/00282]\tTime 0.60 (0.61)\tLoss 10.99 (nan)\n",
      "\t\tcls_loss 0.31 (0.32)\treg_loss 0.04 (0.07)\treco_loss 10.60 (nan)\treco_cls_loss 0.04 (0.15)\n",
      "Epoch: [004][00110/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.52 (0.34)\treg_loss 0.10 (0.07)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.14)\n",
      "Epoch: [004][00120/00282]\tTime 0.60 (0.61)\tLoss 11.78 (nan)\n",
      "\t\tcls_loss 0.30 (0.33)\treg_loss 0.05 (0.07)\treco_loss 11.39 (nan)\treco_cls_loss 0.00 (0.13)\n",
      "Epoch: [004][00130/00282]\tTime 0.60 (0.60)\tLoss 12.42 (nan)\n",
      "\t\tcls_loss 0.37 (0.34)\treg_loss 0.08 (0.07)\treco_loss 11.88 (nan)\treco_cls_loss 0.01 (0.12)\n",
      "Epoch: [004][00140/00282]\tTime 0.62 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.35 (0.34)\treg_loss 0.07 (0.07)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.11)\n",
      "Epoch: [004][00150/00282]\tTime 0.60 (0.61)\tLoss 11.64 (nan)\n",
      "\t\tcls_loss 0.31 (0.34)\treg_loss 0.04 (0.07)\treco_loss 11.24 (nan)\treco_cls_loss 0.00 (0.10)\n",
      "Epoch: [004][00160/00282]\tTime 0.60 (0.60)\tLoss 12.57 (nan)\n",
      "\t\tcls_loss 0.31 (0.33)\treg_loss 0.04 (0.07)\treco_loss 12.18 (nan)\treco_cls_loss 0.00 (0.09)\n",
      "Epoch: [004][00170/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.44 (0.34)\treg_loss 0.06 (0.07)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.09)\n",
      "Epoch: [004][00180/00282]\tTime 0.60 (0.60)\tLoss 12.68 (nan)\n",
      "\t\tcls_loss 0.28 (0.34)\treg_loss 0.04 (0.06)\treco_loss 12.33 (nan)\treco_cls_loss 0.01 (0.08)\n",
      "Epoch: [004][00190/00282]\tTime 0.61 (0.60)\tLoss 11.81 (nan)\n",
      "\t\tcls_loss 0.26 (0.33)\treg_loss 0.06 (0.06)\treco_loss 11.44 (nan)\treco_cls_loss 0.00 (0.08)\n",
      "Epoch: [004][00200/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.36 (0.33)\treg_loss 0.06 (0.06)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.08)\n",
      "Epoch: [004][00210/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.58 (0.35)\treg_loss 0.22 (0.07)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.07)\n",
      "Epoch: [004][00220/00282]\tTime 0.59 (0.60)\tLoss 12.56 (nan)\n",
      "\t\tcls_loss 0.31 (0.34)\treg_loss 0.10 (0.07)\treco_loss 12.06 (nan)\treco_cls_loss 0.00 (0.07)\n",
      "Epoch: [004][00230/00282]\tTime 0.59 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.35 (0.34)\treg_loss 0.04 (0.07)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.07)\n",
      "Epoch: [004][00240/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.35 (0.34)\treg_loss 0.09 (0.07)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.06)\n",
      "Epoch: [004][00250/00282]\tTime 0.59 (0.60)\tLoss 12.35 (nan)\n",
      "\t\tcls_loss 0.27 (0.34)\treg_loss 0.06 (0.07)\treco_loss 11.96 (nan)\treco_cls_loss 0.05 (0.06)\n",
      "Epoch: [004][00260/00282]\tTime 0.60 (0.60)\tLoss 10.97 (nan)\n",
      "\t\tcls_loss 0.30 (0.34)\treg_loss 0.05 (0.07)\treco_loss 10.57 (nan)\treco_cls_loss 0.08 (0.06)\n",
      "Epoch: [004][00270/00282]\tTime 0.60 (0.60)\tLoss 11.59 (nan)\n",
      "\t\tcls_loss 0.26 (0.34)\treg_loss 0.13 (0.07)\treco_loss 11.07 (nan)\treco_cls_loss 0.01 (0.06)\n",
      "Epoch: [004][00280/00282]\tTime 0.59 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.39 (0.34)\treg_loss 0.11 (0.07)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.06)\n",
      "[Train]: Epoch 4 finished with lr=0.00100000\n",
      "\n",
      "\n",
      "[Train]: Epoch 5 started\n",
      "Epoch: [005][00010/00282]\tTime 0.69 (0.69)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.64 (0.64)\treg_loss 0.17 (0.17)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.01)\n",
      "Epoch: [005][00020/00282]\tTime 0.60 (0.64)\tLoss 12.45 (nan)\n",
      "\t\tcls_loss 0.29 (0.46)\treg_loss 0.10 (0.13)\treco_loss 11.96 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [005][00030/00282]\tTime 0.59 (0.63)\tLoss 10.82 (nan)\n",
      "\t\tcls_loss 0.24 (0.39)\treg_loss 0.05 (0.11)\treco_loss 10.47 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [005][00040/00282]\tTime 0.59 (0.62)\tLoss 11.53 (nan)\n",
      "\t\tcls_loss 0.44 (0.40)\treg_loss 0.11 (0.11)\treco_loss 10.87 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [005][00050/00282]\tTime 0.60 (0.62)\tLoss 12.89 (nan)\n",
      "\t\tcls_loss 0.41 (0.40)\treg_loss 0.15 (0.12)\treco_loss 12.13 (nan)\treco_cls_loss 0.54 (0.11)\n",
      "Epoch: [005][00060/00282]\tTime 0.61 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.39)\treg_loss 0.08 (0.11)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.09)\n",
      "Epoch: [005][00070/00282]\tTime 0.59 (0.61)\tLoss 11.76 (nan)\n",
      "\t\tcls_loss 0.31 (0.38)\treg_loss 0.07 (0.10)\treco_loss 11.31 (nan)\treco_cls_loss 0.02 (0.08)\n",
      "Epoch: [005][00080/00282]\tTime 0.59 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.47 (0.39)\treg_loss 0.11 (0.10)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.07)\n",
      "Epoch: [005][00090/00282]\tTime 0.62 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.49 (0.40)\treg_loss 0.15 (0.11)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.07)\n",
      "Epoch: [005][00100/00282]\tTime 0.61 (0.61)\tLoss 12.23 (nan)\n",
      "\t\tcls_loss 0.23 (0.38)\treg_loss 0.06 (0.11)\treco_loss 11.87 (nan)\treco_cls_loss 0.05 (0.06)\n",
      "Epoch: [005][00110/00282]\tTime 0.60 (0.61)\tLoss 11.76 (nan)\n",
      "\t\tcls_loss 0.32 (0.38)\treg_loss 0.07 (0.10)\treco_loss 11.31 (nan)\treco_cls_loss 0.02 (0.06)\n",
      "Epoch: [005][00120/00282]\tTime 0.60 (0.61)\tLoss 12.25 (nan)\n",
      "\t\tcls_loss 0.20 (0.36)\treg_loss 0.03 (0.10)\treco_loss 11.98 (nan)\treco_cls_loss 0.01 (0.06)\n",
      "Epoch: [005][00130/00282]\tTime 0.60 (0.61)\tLoss 11.31 (nan)\n",
      "\t\tcls_loss 0.29 (0.36)\treg_loss 0.04 (0.09)\treco_loss 10.93 (nan)\treco_cls_loss 0.03 (0.05)\n",
      "Epoch: [005][00140/00282]\tTime 0.60 (0.61)\tLoss 10.44 (nan)\n",
      "\t\tcls_loss 0.33 (0.36)\treg_loss 0.05 (0.09)\treco_loss 10.01 (nan)\treco_cls_loss 0.00 (0.05)\n",
      "Epoch: [005][00150/00282]\tTime 0.60 (0.61)\tLoss 12.38 (nan)\n",
      "\t\tcls_loss 0.27 (0.35)\treg_loss 0.07 (0.09)\treco_loss 11.96 (nan)\treco_cls_loss 0.00 (0.05)\n",
      "Epoch: [005][00160/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.37 (0.35)\treg_loss 0.06 (0.09)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.04)\n",
      "Epoch: [005][00170/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.38 (0.35)\treg_loss 0.11 (0.09)\treco_loss nan (nan)\treco_cls_loss 0.26 (0.06)\n",
      "Epoch: [005][00180/00282]\tTime 0.59 (0.61)\tLoss 11.41 (nan)\n",
      "\t\tcls_loss 0.23 (0.35)\treg_loss 0.04 (0.08)\treco_loss 11.10 (nan)\treco_cls_loss 0.01 (0.05)\n",
      "Epoch: [005][00190/00282]\tTime 0.60 (0.61)\tLoss 11.48 (nan)\n",
      "\t\tcls_loss 0.27 (0.34)\treg_loss 0.06 (0.08)\treco_loss 11.06 (nan)\treco_cls_loss 0.38 (0.07)\n",
      "Epoch: [005][00200/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.31 (0.34)\treg_loss 0.05 (0.08)\treco_loss nan (nan)\treco_cls_loss 0.92 (0.11)\n",
      "Epoch: [005][00210/00282]\tTime 0.60 (0.60)\tLoss 12.39 (nan)\n",
      "\t\tcls_loss 0.29 (0.34)\treg_loss 0.06 (0.08)\treco_loss 11.93 (nan)\treco_cls_loss 0.44 (0.13)\n",
      "Epoch: [005][00220/00282]\tTime 0.61 (0.60)\tLoss 12.52 (nan)\n",
      "\t\tcls_loss 0.31 (0.34)\treg_loss 0.05 (0.08)\treco_loss 12.11 (nan)\treco_cls_loss 0.02 (0.12)\n",
      "Epoch: [005][00230/00282]\tTime 0.60 (0.60)\tLoss 12.23 (nan)\n",
      "\t\tcls_loss 0.29 (0.33)\treg_loss 0.10 (0.08)\treco_loss 11.75 (nan)\treco_cls_loss 0.03 (0.12)\n",
      "Epoch: [005][00240/00282]\tTime 0.60 (0.60)\tLoss 9.20 (nan)\n",
      "\t\tcls_loss 0.30 (0.33)\treg_loss 0.05 (0.08)\treco_loss 8.80 (nan)\treco_cls_loss 0.03 (0.12)\n",
      "Epoch: [005][00250/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.42 (0.34)\treg_loss 0.10 (0.08)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.11)\n",
      "Epoch: [005][00260/00282]\tTime 0.60 (0.60)\tLoss 11.62 (nan)\n",
      "\t\tcls_loss 0.33 (0.34)\treg_loss 0.05 (0.08)\treco_loss 11.19 (nan)\treco_cls_loss 0.00 (0.11)\n",
      "Epoch: [005][00270/00282]\tTime 0.60 (0.60)\tLoss 11.26 (nan)\n",
      "\t\tcls_loss 0.34 (0.34)\treg_loss 0.05 (0.08)\treco_loss 10.83 (nan)\treco_cls_loss 0.00 (0.10)\n",
      "Epoch: [005][00280/00282]\tTime 0.59 (0.60)\tLoss 11.39 (nan)\n",
      "\t\tcls_loss 0.21 (0.33)\treg_loss 0.02 (0.08)\treco_loss 11.13 (nan)\treco_cls_loss 0.00 (0.10)\n",
      "[Train]: Epoch 5 finished with lr=0.00097553\n",
      "\n",
      "\n",
      "[Train]: Epoch 6 started\n",
      "Epoch: [006][00010/00282]\tTime 0.68 (0.68)\tLoss 11.98 (11.98)\n",
      "\t\tcls_loss 0.40 (0.40)\treg_loss 0.09 (0.09)\treco_loss 11.39 (11.39)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [006][00020/00282]\tTime 0.60 (0.64)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.36)\treg_loss 0.04 (0.07)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [006][00030/00282]\tTime 0.61 (0.63)\tLoss 11.88 (nan)\n",
      "\t\tcls_loss 0.26 (0.33)\treg_loss 0.03 (0.06)\treco_loss 11.55 (nan)\treco_cls_loss 0.01 (0.00)\n",
      "Epoch: [006][00040/00282]\tTime 0.60 (0.62)\tLoss 11.47 (nan)\n",
      "\t\tcls_loss 0.36 (0.33)\treg_loss 0.04 (0.05)\treco_loss 11.03 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [006][00050/00282]\tTime 0.59 (0.62)\tLoss 12.76 (nan)\n",
      "\t\tcls_loss 0.47 (0.36)\treg_loss 0.08 (0.06)\treco_loss 12.13 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [006][00060/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.36)\treg_loss 0.08 (0.06)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [006][00070/00282]\tTime 0.59 (0.61)\tLoss 11.53 (nan)\n",
      "\t\tcls_loss 0.38 (0.36)\treg_loss 0.05 (0.06)\treco_loss 11.06 (nan)\treco_cls_loss 0.01 (0.00)\n",
      "Epoch: [006][00080/00282]\tTime 0.60 (0.61)\tLoss 11.43 (nan)\n",
      "\t\tcls_loss 0.32 (0.36)\treg_loss 0.03 (0.06)\treco_loss 11.05 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [006][00090/00282]\tTime 0.60 (0.61)\tLoss 11.11 (nan)\n",
      "\t\tcls_loss 0.29 (0.35)\treg_loss 0.04 (0.05)\treco_loss 10.75 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [006][00100/00282]\tTime 0.61 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.37 (0.35)\treg_loss 0.06 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [006][00110/00282]\tTime 0.59 (0.61)\tLoss 12.15 (nan)\n",
      "\t\tcls_loss 0.25 (0.34)\treg_loss 0.03 (0.05)\treco_loss 11.83 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [006][00120/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.34)\treg_loss 0.07 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [006][00130/00282]\tTime 0.60 (0.61)\tLoss 11.65 (nan)\n",
      "\t\tcls_loss 0.29 (0.34)\treg_loss 0.04 (0.05)\treco_loss 11.27 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [006][00140/00282]\tTime 0.60 (0.61)\tLoss 11.17 (nan)\n",
      "\t\tcls_loss 0.25 (0.33)\treg_loss 0.03 (0.05)\treco_loss 10.83 (nan)\treco_cls_loss 0.39 (0.03)\n",
      "Epoch: [006][00150/00282]\tTime 0.59 (0.60)\tLoss 11.99 (nan)\n",
      "\t\tcls_loss 0.29 (0.33)\treg_loss 0.04 (0.05)\treco_loss 11.60 (nan)\treco_cls_loss 0.21 (0.04)\n",
      "Epoch: [006][00160/00282]\tTime 0.59 (0.60)\tLoss 13.14 (nan)\n",
      "\t\tcls_loss 0.32 (0.33)\treg_loss 0.08 (0.05)\treco_loss 12.63 (nan)\treco_cls_loss 0.27 (0.06)\n",
      "Epoch: [006][00170/00282]\tTime 0.60 (0.60)\tLoss 10.85 (nan)\n",
      "\t\tcls_loss 0.25 (0.32)\treg_loss 0.06 (0.05)\treco_loss 10.49 (nan)\treco_cls_loss 0.01 (0.05)\n",
      "Epoch: [006][00180/00282]\tTime 0.59 (0.60)\tLoss 12.48 (nan)\n",
      "\t\tcls_loss 0.27 (0.32)\treg_loss 0.04 (0.05)\treco_loss 12.14 (nan)\treco_cls_loss 0.01 (0.05)\n",
      "Epoch: [006][00190/00282]\tTime 0.59 (0.60)\tLoss 11.35 (nan)\n",
      "\t\tcls_loss 0.24 (0.32)\treg_loss 0.04 (0.05)\treco_loss 11.04 (nan)\treco_cls_loss 0.03 (0.05)\n",
      "Epoch: [006][00200/00282]\tTime 0.61 (0.60)\tLoss 11.81 (nan)\n",
      "\t\tcls_loss 0.31 (0.31)\treg_loss 0.03 (0.05)\treco_loss 11.45 (nan)\treco_cls_loss 0.01 (0.05)\n",
      "Epoch: [006][00210/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.35 (0.32)\treg_loss 0.11 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.05)\n",
      "Epoch: [006][00220/00282]\tTime 0.60 (0.60)\tLoss 12.38 (nan)\n",
      "\t\tcls_loss 0.27 (0.31)\treg_loss 0.04 (0.05)\treco_loss 12.05 (nan)\treco_cls_loss 0.01 (0.04)\n",
      "Epoch: [006][00230/00282]\tTime 0.60 (0.60)\tLoss 10.86 (nan)\n",
      "\t\tcls_loss 0.35 (0.32)\treg_loss 0.11 (0.05)\treco_loss 10.30 (nan)\treco_cls_loss 0.01 (0.04)\n",
      "Epoch: [006][00240/00282]\tTime 0.62 (0.60)\tLoss 10.97 (nan)\n",
      "\t\tcls_loss 0.27 (0.31)\treg_loss 0.06 (0.05)\treco_loss 10.57 (nan)\treco_cls_loss 0.02 (0.04)\n",
      "Epoch: [006][00250/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.43 (0.32)\treg_loss 0.06 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.04)\n",
      "Epoch: [006][00260/00282]\tTime 0.61 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.32)\treg_loss 0.04 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.04)\n",
      "Epoch: [006][00270/00282]\tTime 0.59 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.32)\treg_loss 0.04 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.04)\n",
      "Epoch: [006][00280/00282]\tTime 0.61 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.41 (0.32)\treg_loss 0.06 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.04)\n",
      "[Train]: Epoch 6 finished with lr=0.00090451\n",
      "\n",
      "\n",
      "[Train]: Epoch 7 started\n",
      "Epoch: [007][00010/00282]\tTime 0.69 (0.69)\tLoss 11.51 (11.51)\n",
      "\t\tcls_loss 0.26 (0.26)\treg_loss 0.03 (0.03)\treco_loss 11.20 (11.20)\treco_cls_loss 0.01 (0.01)\n",
      "Epoch: [007][00020/00282]\tTime 0.61 (0.65)\tLoss 12.55 (12.03)\n",
      "\t\tcls_loss 0.30 (0.28)\treg_loss 0.04 (0.03)\treco_loss 12.17 (11.69)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00030/00282]\tTime 0.60 (0.63)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.29)\treg_loss 0.04 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.03 (0.01)\n",
      "Epoch: [007][00040/00282]\tTime 0.61 (0.63)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.35 (0.31)\treg_loss 0.06 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00050/00282]\tTime 0.61 (0.62)\tLoss 12.14 (nan)\n",
      "\t\tcls_loss 0.30 (0.30)\treg_loss 0.06 (0.05)\treco_loss 11.72 (nan)\treco_cls_loss 0.05 (0.02)\n",
      "Epoch: [007][00060/00282]\tTime 0.61 (0.62)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.50 (0.34)\treg_loss 0.10 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.02)\n",
      "Epoch: [007][00070/00282]\tTime 0.60 (0.62)\tLoss 11.77 (nan)\n",
      "\t\tcls_loss 0.36 (0.34)\treg_loss 0.06 (0.05)\treco_loss 11.30 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00080/00282]\tTime 0.60 (0.61)\tLoss 11.77 (nan)\n",
      "\t\tcls_loss 0.23 (0.33)\treg_loss 0.03 (0.05)\treco_loss 11.47 (nan)\treco_cls_loss 0.02 (0.01)\n",
      "Epoch: [007][00090/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.33)\treg_loss 0.05 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00100/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.33)\treg_loss 0.05 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00110/00282]\tTime 0.61 (0.61)\tLoss 11.81 (nan)\n",
      "\t\tcls_loss 0.29 (0.32)\treg_loss 0.07 (0.05)\treco_loss 11.39 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00120/00282]\tTime 0.60 (0.61)\tLoss 12.19 (nan)\n",
      "\t\tcls_loss 0.35 (0.33)\treg_loss 0.05 (0.05)\treco_loss 11.74 (nan)\treco_cls_loss 0.01 (0.01)\n",
      "Epoch: [007][00130/00282]\tTime 0.60 (0.61)\tLoss 12.06 (nan)\n",
      "\t\tcls_loss 0.32 (0.33)\treg_loss 0.03 (0.05)\treco_loss 11.68 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00140/00282]\tTime 0.61 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.33)\treg_loss 0.05 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00150/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.33)\treg_loss 0.07 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00160/00282]\tTime 0.60 (0.61)\tLoss 11.15 (nan)\n",
      "\t\tcls_loss 0.21 (0.32)\treg_loss 0.03 (0.05)\treco_loss 10.87 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00170/00282]\tTime 0.60 (0.61)\tLoss 11.10 (nan)\n",
      "\t\tcls_loss 0.29 (0.32)\treg_loss 0.05 (0.05)\treco_loss 10.71 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00180/00282]\tTime 0.61 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.32)\treg_loss 0.04 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00190/00282]\tTime 0.60 (0.61)\tLoss 12.04 (nan)\n",
      "\t\tcls_loss 0.29 (0.32)\treg_loss 0.04 (0.05)\treco_loss 11.66 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00200/00282]\tTime 0.59 (0.61)\tLoss 12.65 (nan)\n",
      "\t\tcls_loss 0.29 (0.32)\treg_loss 0.05 (0.05)\treco_loss 12.25 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [007][00210/00282]\tTime 0.62 (0.61)\tLoss 11.74 (nan)\n",
      "\t\tcls_loss 0.31 (0.32)\treg_loss 0.04 (0.05)\treco_loss 11.34 (nan)\treco_cls_loss 0.01 (0.01)\n",
      "Epoch: [007][00220/00282]\tTime 0.60 (0.61)\tLoss 10.47 (nan)\n",
      "\t\tcls_loss 0.27 (0.31)\treg_loss 0.04 (0.05)\treco_loss 10.01 (nan)\treco_cls_loss 0.99 (0.05)\n",
      "Epoch: [007][00230/00282]\tTime 0.59 (0.61)\tLoss 11.59 (nan)\n",
      "\t\tcls_loss 0.31 (0.31)\treg_loss 0.05 (0.05)\treco_loss 11.18 (nan)\treco_cls_loss 0.00 (0.05)\n",
      "Epoch: [007][00240/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.38 (0.32)\treg_loss 0.05 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.17 (0.05)\n",
      "Epoch: [007][00250/00282]\tTime 0.60 (0.61)\tLoss 13.10 (nan)\n",
      "\t\tcls_loss 0.36 (0.32)\treg_loss 0.04 (0.05)\treco_loss 12.67 (nan)\treco_cls_loss 0.03 (0.05)\n",
      "Epoch: [007][00260/00282]\tTime 0.60 (0.61)\tLoss 12.50 (nan)\n",
      "\t\tcls_loss 0.32 (0.32)\treg_loss 0.05 (0.05)\treco_loss 12.02 (nan)\treco_cls_loss 0.66 (0.08)\n",
      "Epoch: [007][00270/00282]\tTime 0.62 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.38 (0.32)\treg_loss 0.04 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.08 (0.08)\n",
      "Epoch: [007][00280/00282]\tTime 0.61 (0.61)\tLoss 12.65 (nan)\n",
      "\t\tcls_loss 0.31 (0.32)\treg_loss 0.06 (0.05)\treco_loss 12.15 (nan)\treco_cls_loss 0.63 (0.10)\n",
      "[Train]: Epoch 7 finished with lr=0.00079389\n",
      "\n",
      "\n",
      "[Train]: Epoch 8 started\n",
      "Epoch: [008][00010/00282]\tTime 0.69 (0.69)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.35 (0.35)\treg_loss 0.06 (0.06)\treco_loss nan (nan)\treco_cls_loss 0.06 (0.06)\n",
      "Epoch: [008][00020/00282]\tTime 0.61 (0.65)\tLoss 12.45 (nan)\n",
      "\t\tcls_loss 0.30 (0.32)\treg_loss 0.03 (0.04)\treco_loss 12.10 (nan)\treco_cls_loss 0.01 (0.03)\n",
      "Epoch: [008][00030/00282]\tTime 0.61 (0.63)\tLoss 11.09 (nan)\n",
      "\t\tcls_loss 0.28 (0.31)\treg_loss 0.03 (0.04)\treco_loss 10.74 (nan)\treco_cls_loss 0.03 (0.03)\n",
      "Epoch: [008][00040/00282]\tTime 0.64 (0.64)\tLoss 11.79 (nan)\n",
      "\t\tcls_loss 0.29 (0.31)\treg_loss 0.04 (0.04)\treco_loss 11.42 (nan)\treco_cls_loss 0.00 (0.02)\n",
      "Epoch: [008][00050/00282]\tTime 0.61 (0.63)\tLoss 12.04 (nan)\n",
      "\t\tcls_loss 0.26 (0.30)\treg_loss 0.03 (0.04)\treco_loss 11.72 (nan)\treco_cls_loss 0.00 (0.02)\n",
      "Epoch: [008][00060/00282]\tTime 0.60 (0.63)\tLoss 12.20 (nan)\n",
      "\t\tcls_loss 0.28 (0.29)\treg_loss 0.03 (0.04)\treco_loss 11.86 (nan)\treco_cls_loss 0.01 (0.02)\n",
      "Epoch: [008][00070/00282]\tTime 0.61 (0.62)\tLoss 12.02 (nan)\n",
      "\t\tcls_loss 0.30 (0.29)\treg_loss 0.04 (0.04)\treco_loss 11.64 (nan)\treco_cls_loss 0.01 (0.02)\n",
      "Epoch: [008][00080/00282]\tTime 0.60 (0.62)\tLoss 11.32 (nan)\n",
      "\t\tcls_loss 0.29 (0.29)\treg_loss 0.04 (0.04)\treco_loss 10.95 (nan)\treco_cls_loss 0.01 (0.02)\n",
      "Epoch: [008][00090/00282]\tTime 0.59 (0.62)\tLoss 11.41 (nan)\n",
      "\t\tcls_loss 0.22 (0.29)\treg_loss 0.04 (0.04)\treco_loss 11.11 (nan)\treco_cls_loss 0.01 (0.01)\n",
      "Epoch: [008][00100/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.37 (0.29)\treg_loss 0.06 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.02 (0.01)\n",
      "Epoch: [008][00110/00282]\tTime 0.60 (0.61)\tLoss 12.69 (nan)\n",
      "\t\tcls_loss 0.35 (0.30)\treg_loss 0.08 (0.04)\treco_loss 12.17 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [008][00120/00282]\tTime 0.60 (0.61)\tLoss 11.10 (nan)\n",
      "\t\tcls_loss 0.34 (0.30)\treg_loss 0.10 (0.05)\treco_loss 10.54 (nan)\treco_cls_loss 0.23 (0.03)\n",
      "Epoch: [008][00130/00282]\tTime 0.60 (0.61)\tLoss 11.04 (nan)\n",
      "\t\tcls_loss 0.29 (0.30)\treg_loss 0.04 (0.05)\treco_loss 10.67 (nan)\treco_cls_loss 0.01 (0.03)\n",
      "Epoch: [008][00140/00282]\tTime 0.60 (0.61)\tLoss 11.84 (nan)\n",
      "\t\tcls_loss 0.21 (0.30)\treg_loss 0.03 (0.05)\treco_loss 11.57 (nan)\treco_cls_loss 0.01 (0.03)\n",
      "Epoch: [008][00150/00282]\tTime 0.60 (0.61)\tLoss 11.51 (nan)\n",
      "\t\tcls_loss 0.30 (0.30)\treg_loss 0.05 (0.05)\treco_loss 11.11 (nan)\treco_cls_loss 0.02 (0.03)\n",
      "Epoch: [008][00160/00282]\tTime 0.61 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.35 (0.30)\treg_loss 0.04 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [008][00170/00282]\tTime 0.60 (0.61)\tLoss 11.79 (nan)\n",
      "\t\tcls_loss 0.29 (0.30)\treg_loss 0.05 (0.05)\treco_loss 11.40 (nan)\treco_cls_loss 0.01 (0.03)\n",
      "Epoch: [008][00180/00282]\tTime 0.60 (0.61)\tLoss 12.43 (nan)\n",
      "\t\tcls_loss 0.34 (0.30)\treg_loss 0.06 (0.05)\treco_loss 11.97 (nan)\treco_cls_loss 0.04 (0.03)\n",
      "Epoch: [008][00190/00282]\tTime 0.60 (0.61)\tLoss 12.96 (nan)\n",
      "\t\tcls_loss 0.29 (0.30)\treg_loss 0.06 (0.05)\treco_loss 12.56 (nan)\treco_cls_loss 0.02 (0.03)\n",
      "Epoch: [008][00200/00282]\tTime 0.60 (0.61)\tLoss 12.50 (nan)\n",
      "\t\tcls_loss 0.32 (0.30)\treg_loss 0.06 (0.05)\treco_loss 12.06 (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [008][00210/00282]\tTime 0.60 (0.61)\tLoss 12.82 (nan)\n",
      "\t\tcls_loss 0.30 (0.30)\treg_loss 0.03 (0.05)\treco_loss 12.47 (nan)\treco_cls_loss 0.00 (0.02)\n",
      "Epoch: [008][00220/00282]\tTime 0.61 (0.61)\tLoss 14.11 (nan)\n",
      "\t\tcls_loss 0.30 (0.30)\treg_loss 0.02 (0.05)\treco_loss 13.77 (nan)\treco_cls_loss 0.01 (0.02)\n",
      "Epoch: [008][00230/00282]\tTime 0.61 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.30)\treg_loss 0.04 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.02)\n",
      "Epoch: [008][00240/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.30)\treg_loss 0.05 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.05 (0.02)\n",
      "Epoch: [008][00250/00282]\tTime 0.60 (0.61)\tLoss 10.68 (nan)\n",
      "\t\tcls_loss 0.32 (0.30)\treg_loss 0.06 (0.05)\treco_loss 10.24 (nan)\treco_cls_loss 0.00 (0.02)\n",
      "Epoch: [008][00260/00282]\tTime 0.60 (0.61)\tLoss 11.60 (nan)\n",
      "\t\tcls_loss 0.26 (0.30)\treg_loss 0.04 (0.05)\treco_loss 11.25 (nan)\treco_cls_loss 0.11 (0.03)\n",
      "Epoch: [008][00270/00282]\tTime 0.60 (0.61)\tLoss 11.49 (nan)\n",
      "\t\tcls_loss 0.25 (0.30)\treg_loss 0.02 (0.04)\treco_loss 11.20 (nan)\treco_cls_loss 0.01 (0.03)\n",
      "Epoch: [008][00280/00282]\tTime 0.59 (0.61)\tLoss 12.90 (nan)\n",
      "\t\tcls_loss 0.26 (0.30)\treg_loss 0.03 (0.04)\treco_loss 12.59 (nan)\treco_cls_loss 0.00 (0.02)\n",
      "[Train]: Epoch 8 finished with lr=0.00065451\n",
      "\n",
      "\n",
      "[Train]: Epoch 9 started\n",
      "Epoch: [009][00010/00282]\tTime 0.71 (0.71)\tLoss 11.47 (11.47)\n",
      "\t\tcls_loss 0.29 (0.29)\treg_loss 0.03 (0.03)\treco_loss 11.11 (11.11)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [009][00020/00282]\tTime 0.60 (0.65)\tLoss 11.78 (11.62)\n",
      "\t\tcls_loss 0.21 (0.25)\treg_loss 0.03 (0.03)\treco_loss 11.50 (11.30)\treco_cls_loss 0.13 (0.07)\n",
      "Epoch: [009][00030/00282]\tTime 0.61 (0.64)\tLoss 11.34 (11.53)\n",
      "\t\tcls_loss 0.25 (0.25)\treg_loss 0.03 (0.03)\treco_loss 11.03 (11.21)\treco_cls_loss 0.16 (0.10)\n",
      "Epoch: [009][00040/00282]\tTime 0.61 (0.63)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.27)\treg_loss 0.06 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.08)\n",
      "Epoch: [009][00050/00282]\tTime 0.61 (0.62)\tLoss 10.36 (nan)\n",
      "\t\tcls_loss 0.30 (0.27)\treg_loss 0.04 (0.04)\treco_loss 9.98 (nan)\treco_cls_loss 0.00 (0.06)\n",
      "Epoch: [009][00060/00282]\tTime 0.61 (0.62)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.28)\treg_loss 0.05 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.05)\n",
      "Epoch: [009][00070/00282]\tTime 0.60 (0.62)\tLoss 10.81 (nan)\n",
      "\t\tcls_loss 0.30 (0.29)\treg_loss 0.04 (0.04)\treco_loss 10.44 (nan)\treco_cls_loss 0.00 (0.04)\n",
      "Epoch: [009][00080/00282]\tTime 0.60 (0.62)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.36 (0.29)\treg_loss 0.02 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.04)\n",
      "Epoch: [009][00090/00282]\tTime 0.60 (0.61)\tLoss 11.64 (nan)\n",
      "\t\tcls_loss 0.27 (0.29)\treg_loss 0.07 (0.04)\treco_loss 11.24 (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [009][00100/00282]\tTime 0.61 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.38 (0.30)\treg_loss 0.06 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [009][00110/00282]\tTime 0.62 (0.61)\tLoss 9.95 (nan)\n",
      "\t\tcls_loss 0.30 (0.30)\treg_loss 0.03 (0.04)\treco_loss 9.59 (nan)\treco_cls_loss 0.01 (0.03)\n",
      "Epoch: [009][00120/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.41 (0.31)\treg_loss 0.06 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [009][00130/00282]\tTime 0.62 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.52 (0.33)\treg_loss 0.10 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.39 (0.06)\n",
      "Epoch: [009][00140/00282]\tTime 0.61 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.31 (0.32)\treg_loss 0.02 (0.05)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.05)\n",
      "Epoch: [009][00150/00282]\tTime 0.61 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.32)\treg_loss 0.02 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.05)\n",
      "Epoch: [009][00160/00282]\tTime 0.61 (0.61)\tLoss 12.06 (nan)\n",
      "\t\tcls_loss 0.30 (0.32)\treg_loss 0.05 (0.05)\treco_loss 11.65 (nan)\treco_cls_loss 0.00 (0.05)\n",
      "Epoch: [009][00170/00282]\tTime 0.61 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.31 (0.32)\treg_loss 0.03 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.04)\n",
      "Epoch: [009][00180/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.31 (0.32)\treg_loss 0.03 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.04)\n",
      "Epoch: [009][00190/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.36 (0.32)\treg_loss 0.04 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.04)\n",
      "Epoch: [009][00200/00282]\tTime 0.60 (0.61)\tLoss 13.15 (nan)\n",
      "\t\tcls_loss 0.27 (0.32)\treg_loss 0.04 (0.04)\treco_loss 12.80 (nan)\treco_cls_loss 0.00 (0.04)\n",
      "Epoch: [009][00210/00282]\tTime 0.60 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.35 (0.32)\treg_loss 0.03 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [009][00220/00282]\tTime 0.61 (0.61)\tLoss 11.56 (nan)\n",
      "\t\tcls_loss 0.33 (0.32)\treg_loss 0.03 (0.04)\treco_loss 11.18 (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [009][00230/00282]\tTime 0.58 (0.61)\tLoss 10.75 (nan)\n",
      "\t\tcls_loss 0.26 (0.32)\treg_loss 0.03 (0.04)\treco_loss 10.42 (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [009][00240/00282]\tTime 0.58 (0.61)\tLoss 12.40 (nan)\n",
      "\t\tcls_loss 0.31 (0.32)\treg_loss 0.07 (0.04)\treco_loss 11.96 (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [009][00250/00282]\tTime 0.59 (0.61)\tLoss 12.55 (nan)\n",
      "\t\tcls_loss 0.22 (0.32)\treg_loss 0.03 (0.04)\treco_loss 12.28 (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [009][00260/00282]\tTime 0.60 (0.61)\tLoss 11.19 (nan)\n",
      "\t\tcls_loss 0.33 (0.32)\treg_loss 0.02 (0.04)\treco_loss 10.81 (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [009][00270/00282]\tTime 0.59 (0.61)\tLoss 12.42 (nan)\n",
      "\t\tcls_loss 0.30 (0.32)\treg_loss 0.04 (0.04)\treco_loss 12.05 (nan)\treco_cls_loss 0.00 (0.03)\n",
      "Epoch: [009][00280/00282]\tTime 0.58 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.32)\treg_loss 0.03 (0.04)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.03)\n",
      "[Train]: Epoch 9 finished with lr=0.00050000\n",
      "\n",
      "\n",
      "[Train]: Epoch 10 started\n",
      "Epoch: [010][00010/00282]\tTime 0.68 (0.68)\tLoss 10.45 (10.45)\n",
      "\t\tcls_loss 0.33 (0.33)\treg_loss 0.02 (0.02)\treco_loss 10.07 (10.07)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00020/00282]\tTime 0.59 (0.63)\tLoss 11.75 (11.10)\n",
      "\t\tcls_loss 0.27 (0.30)\treg_loss 0.04 (0.03)\treco_loss 11.40 (10.73)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00030/00282]\tTime 0.59 (0.62)\tLoss 12.37 (11.52)\n",
      "\t\tcls_loss 0.32 (0.31)\treg_loss 0.05 (0.04)\treco_loss 11.95 (11.14)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00040/00282]\tTime 0.59 (0.61)\tLoss 11.47 (11.51)\n",
      "\t\tcls_loss 0.29 (0.30)\treg_loss 0.03 (0.04)\treco_loss 11.13 (11.13)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00050/00282]\tTime 0.59 (0.61)\tLoss 11.57 (11.52)\n",
      "\t\tcls_loss 0.24 (0.29)\treg_loss 0.01 (0.03)\treco_loss 11.31 (11.17)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00060/00282]\tTime 0.58 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.29)\treg_loss 0.04 (0.03)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.00)\n",
      "Epoch: [010][00070/00282]\tTime 0.58 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.30)\treg_loss 0.03 (0.03)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00080/00282]\tTime 0.57 (0.60)\tLoss 11.62 (nan)\n",
      "\t\tcls_loss 0.29 (0.30)\treg_loss 0.03 (0.03)\treco_loss 11.27 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00090/00282]\tTime 0.58 (0.59)\tLoss 10.35 (nan)\n",
      "\t\tcls_loss 0.29 (0.30)\treg_loss 0.03 (0.03)\treco_loss 10.01 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00100/00282]\tTime 0.60 (0.59)\tLoss 12.34 (nan)\n",
      "\t\tcls_loss 0.25 (0.29)\treg_loss 0.03 (0.03)\treco_loss 12.03 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00110/00282]\tTime 0.59 (0.59)\tLoss 11.88 (nan)\n",
      "\t\tcls_loss 0.26 (0.29)\treg_loss 0.02 (0.03)\treco_loss 11.57 (nan)\treco_cls_loss 0.08 (0.01)\n",
      "Epoch: [010][00120/00282]\tTime 0.60 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.36 (0.30)\treg_loss 0.05 (0.03)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [010][00130/00282]\tTime 0.60 (0.60)\tLoss 10.18 (nan)\n",
      "\t\tcls_loss 0.30 (0.30)\treg_loss 0.03 (0.03)\treco_loss 9.82 (nan)\treco_cls_loss 0.01 (0.01)\n",
      "Epoch: [010][00140/00282]\tTime 0.60 (0.60)\tLoss 11.46 (nan)\n",
      "\t\tcls_loss 0.28 (0.29)\treg_loss 0.03 (0.03)\treco_loss 11.11 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [010][00150/00282]\tTime 0.59 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.31 (0.30)\treg_loss 0.03 (0.03)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [010][00160/00282]\tTime 0.58 (0.59)\tLoss 11.81 (nan)\n",
      "\t\tcls_loss 0.29 (0.30)\treg_loss 0.04 (0.03)\treco_loss 11.44 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [010][00170/00282]\tTime 0.59 (0.59)\tLoss 11.84 (nan)\n",
      "\t\tcls_loss 0.30 (0.30)\treg_loss 0.03 (0.03)\treco_loss 11.49 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [010][00180/00282]\tTime 0.62 (0.60)\tLoss 12.71 (nan)\n",
      "\t\tcls_loss 0.21 (0.29)\treg_loss 0.02 (0.03)\treco_loss 12.45 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [010][00190/00282]\tTime 0.61 (0.60)\tLoss 11.42 (nan)\n",
      "\t\tcls_loss 0.26 (0.29)\treg_loss 0.05 (0.03)\treco_loss 11.05 (nan)\treco_cls_loss 0.01 (0.01)\n",
      "Epoch: [010][00200/00282]\tTime 0.57 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.29)\treg_loss 0.02 (0.03)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [010][00210/00282]\tTime 0.59 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.29)\treg_loss 0.03 (0.03)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [010][00220/00282]\tTime 0.62 (0.60)\tLoss 12.15 (nan)\n",
      "\t\tcls_loss 0.28 (0.29)\treg_loss 0.04 (0.03)\treco_loss 11.79 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [010][00230/00282]\tTime 0.59 (0.60)\tLoss 12.30 (nan)\n",
      "\t\tcls_loss 0.25 (0.29)\treg_loss 0.01 (0.03)\treco_loss 12.03 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00240/00282]\tTime 0.59 (0.59)\tLoss 11.58 (nan)\n",
      "\t\tcls_loss 0.29 (0.29)\treg_loss 0.05 (0.03)\treco_loss 11.20 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00250/00282]\tTime 0.58 (0.59)\tLoss 12.91 (nan)\n",
      "\t\tcls_loss 0.28 (0.29)\treg_loss 0.02 (0.03)\treco_loss 12.58 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00260/00282]\tTime 0.56 (0.59)\tLoss 10.72 (nan)\n",
      "\t\tcls_loss 0.31 (0.29)\treg_loss 0.03 (0.03)\treco_loss 10.34 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00270/00282]\tTime 0.56 (0.59)\tLoss 12.29 (nan)\n",
      "\t\tcls_loss 0.26 (0.29)\treg_loss 0.08 (0.03)\treco_loss 11.88 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [010][00280/00282]\tTime 0.59 (0.59)\tLoss 12.03 (nan)\n",
      "\t\tcls_loss 0.28 (0.29)\treg_loss 0.02 (0.03)\treco_loss 11.72 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "[Train]: Epoch 10 finished with lr=0.00034550\n",
      "\n",
      "\n",
      "[Train]: Epoch 11 started\n",
      "Epoch: [011][00010/00282]\tTime 0.68 (0.68)\tLoss 11.87 (11.87)\n",
      "\t\tcls_loss 0.17 (0.17)\treg_loss 0.02 (0.02)\treco_loss 11.66 (11.66)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00020/00282]\tTime 0.59 (0.63)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.35 (0.26)\treg_loss 0.03 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00030/00282]\tTime 0.59 (0.62)\tLoss 10.95 (nan)\n",
      "\t\tcls_loss 0.28 (0.27)\treg_loss 0.02 (0.02)\treco_loss 10.62 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00040/00282]\tTime 0.61 (0.62)\tLoss 12.25 (nan)\n",
      "\t\tcls_loss 0.28 (0.27)\treg_loss 0.02 (0.02)\treco_loss 11.93 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00050/00282]\tTime 0.57 (0.61)\tLoss 11.97 (nan)\n",
      "\t\tcls_loss 0.27 (0.27)\treg_loss 0.03 (0.02)\treco_loss 11.64 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00060/00282]\tTime 0.56 (0.60)\tLoss 13.62 (nan)\n",
      "\t\tcls_loss 0.26 (0.27)\treg_loss 0.02 (0.02)\treco_loss 13.32 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00070/00282]\tTime 0.56 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.31 (0.27)\treg_loss 0.02 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00080/00282]\tTime 0.56 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.28)\treg_loss 0.03 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00090/00282]\tTime 0.57 (0.59)\tLoss 12.47 (nan)\n",
      "\t\tcls_loss 0.25 (0.28)\treg_loss 0.03 (0.02)\treco_loss 12.15 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00100/00282]\tTime 0.57 (0.59)\tLoss 11.19 (nan)\n",
      "\t\tcls_loss 0.22 (0.27)\treg_loss 0.02 (0.02)\treco_loss 10.93 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00110/00282]\tTime 0.57 (0.58)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.28)\treg_loss 0.04 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00120/00282]\tTime 0.57 (0.58)\tLoss 10.23 (nan)\n",
      "\t\tcls_loss 0.30 (0.28)\treg_loss 0.03 (0.02)\treco_loss 9.86 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00130/00282]\tTime 0.58 (0.58)\tLoss 11.48 (nan)\n",
      "\t\tcls_loss 0.25 (0.28)\treg_loss 0.02 (0.02)\treco_loss 11.20 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00140/00282]\tTime 0.61 (0.59)\tLoss 11.81 (nan)\n",
      "\t\tcls_loss 0.21 (0.27)\treg_loss 0.02 (0.02)\treco_loss 11.56 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00150/00282]\tTime 0.61 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.28)\treg_loss 0.04 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00160/00282]\tTime 0.60 (0.59)\tLoss 11.78 (nan)\n",
      "\t\tcls_loss 0.29 (0.28)\treg_loss 0.02 (0.02)\treco_loss 11.45 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00170/00282]\tTime 0.59 (0.59)\tLoss 11.59 (nan)\n",
      "\t\tcls_loss 0.24 (0.28)\treg_loss 0.02 (0.02)\treco_loss 11.31 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00180/00282]\tTime 0.59 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.28)\treg_loss 0.03 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.01 (0.00)\n",
      "Epoch: [011][00190/00282]\tTime 0.60 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.31 (0.28)\treg_loss 0.02 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00200/00282]\tTime 0.60 (0.59)\tLoss 13.08 (nan)\n",
      "\t\tcls_loss 0.28 (0.28)\treg_loss 0.02 (0.02)\treco_loss 12.77 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00210/00282]\tTime 0.61 (0.59)\tLoss 10.98 (nan)\n",
      "\t\tcls_loss 0.28 (0.28)\treg_loss 0.02 (0.02)\treco_loss 10.65 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00220/00282]\tTime 0.60 (0.59)\tLoss 11.00 (nan)\n",
      "\t\tcls_loss 0.30 (0.28)\treg_loss 0.02 (0.02)\treco_loss 10.65 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00230/00282]\tTime 0.61 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.28)\treg_loss 0.01 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00240/00282]\tTime 0.60 (0.59)\tLoss 11.26 (nan)\n",
      "\t\tcls_loss 0.32 (0.29)\treg_loss 0.03 (0.02)\treco_loss 10.87 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00250/00282]\tTime 0.60 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.29)\treg_loss 0.03 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00260/00282]\tTime 0.59 (0.59)\tLoss 10.37 (nan)\n",
      "\t\tcls_loss 0.30 (0.29)\treg_loss 0.04 (0.02)\treco_loss 9.98 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00270/00282]\tTime 0.60 (0.59)\tLoss 11.87 (nan)\n",
      "\t\tcls_loss 0.28 (0.29)\treg_loss 0.02 (0.02)\treco_loss 11.56 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [011][00280/00282]\tTime 0.59 (0.59)\tLoss 11.74 (nan)\n",
      "\t\tcls_loss 0.33 (0.29)\treg_loss 0.05 (0.03)\treco_loss 11.30 (nan)\treco_cls_loss 0.21 (0.01)\n",
      "[Train]: Epoch 11 finished with lr=0.00020612\n",
      "\n",
      "\n",
      "[Train]: Epoch 12 started\n",
      "Epoch: [012][00010/00282]\tTime 0.68 (0.68)\tLoss 11.14 (11.14)\n",
      "\t\tcls_loss 0.28 (0.28)\treg_loss 0.01 (0.01)\treco_loss 10.83 (10.83)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00020/00282]\tTime 0.60 (0.64)\tLoss 11.10 (11.12)\n",
      "\t\tcls_loss 0.27 (0.28)\treg_loss 0.01 (0.01)\treco_loss 10.80 (10.82)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00030/00282]\tTime 0.60 (0.63)\tLoss 11.81 (11.35)\n",
      "\t\tcls_loss 0.31 (0.29)\treg_loss 0.03 (0.02)\treco_loss 11.44 (11.02)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00040/00282]\tTime 0.60 (0.62)\tLoss 11.81 (11.47)\n",
      "\t\tcls_loss 0.20 (0.27)\treg_loss 0.01 (0.02)\treco_loss 11.59 (11.17)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00050/00282]\tTime 0.60 (0.62)\tLoss 10.26 (11.22)\n",
      "\t\tcls_loss 0.40 (0.29)\treg_loss 0.03 (0.02)\treco_loss 9.81 (10.89)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00060/00282]\tTime 0.60 (0.61)\tLoss 10.63 (11.13)\n",
      "\t\tcls_loss 0.27 (0.29)\treg_loss 0.03 (0.02)\treco_loss 10.29 (10.79)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00070/00282]\tTime 0.59 (0.61)\tLoss 12.14 (11.27)\n",
      "\t\tcls_loss 0.27 (0.29)\treg_loss 0.02 (0.02)\treco_loss 11.84 (10.94)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00080/00282]\tTime 0.57 (0.61)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.31 (0.29)\treg_loss 0.03 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00090/00282]\tTime 0.55 (0.60)\tLoss 12.48 (nan)\n",
      "\t\tcls_loss 0.22 (0.28)\treg_loss 0.01 (0.02)\treco_loss 12.24 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00100/00282]\tTime 0.56 (0.60)\tLoss 10.89 (nan)\n",
      "\t\tcls_loss 0.24 (0.28)\treg_loss 0.02 (0.02)\treco_loss 10.61 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00110/00282]\tTime 0.56 (0.59)\tLoss 11.55 (nan)\n",
      "\t\tcls_loss 0.28 (0.28)\treg_loss 0.02 (0.02)\treco_loss 11.24 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00120/00282]\tTime 0.56 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.28)\treg_loss 0.02 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00130/00282]\tTime 0.56 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.29)\treg_loss 0.03 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00140/00282]\tTime 0.57 (0.59)\tLoss 12.58 (nan)\n",
      "\t\tcls_loss 0.24 (0.28)\treg_loss 0.01 (0.02)\treco_loss 12.31 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00150/00282]\tTime 0.57 (0.58)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.35 (0.29)\treg_loss 0.07 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00160/00282]\tTime 0.57 (0.58)\tLoss 11.32 (nan)\n",
      "\t\tcls_loss 0.29 (0.29)\treg_loss 0.02 (0.02)\treco_loss 10.99 (nan)\treco_cls_loss 0.04 (0.00)\n",
      "Epoch: [012][00170/00282]\tTime 0.58 (0.58)\tLoss 11.34 (nan)\n",
      "\t\tcls_loss 0.24 (0.29)\treg_loss 0.02 (0.02)\treco_loss 11.06 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00180/00282]\tTime 0.57 (0.58)\tLoss 13.41 (nan)\n",
      "\t\tcls_loss 0.29 (0.29)\treg_loss 0.03 (0.02)\treco_loss 13.07 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00190/00282]\tTime 0.56 (0.58)\tLoss 11.87 (nan)\n",
      "\t\tcls_loss 0.28 (0.28)\treg_loss 0.03 (0.02)\treco_loss 11.53 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00200/00282]\tTime 0.57 (0.58)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.29)\treg_loss 0.03 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00210/00282]\tTime 0.56 (0.58)\tLoss 11.73 (nan)\n",
      "\t\tcls_loss 0.25 (0.29)\treg_loss 0.02 (0.02)\treco_loss 11.44 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00220/00282]\tTime 0.56 (0.58)\tLoss 10.96 (nan)\n",
      "\t\tcls_loss 0.28 (0.29)\treg_loss 0.02 (0.02)\treco_loss 10.64 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00230/00282]\tTime 0.56 (0.58)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.30 (0.29)\treg_loss 0.01 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00240/00282]\tTime 0.56 (0.58)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.29)\treg_loss 0.02 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00250/00282]\tTime 0.56 (0.58)\tLoss 12.27 (nan)\n",
      "\t\tcls_loss 0.31 (0.29)\treg_loss 0.01 (0.02)\treco_loss 11.93 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00260/00282]\tTime 0.56 (0.58)\tLoss 11.43 (nan)\n",
      "\t\tcls_loss 0.26 (0.29)\treg_loss 0.01 (0.02)\treco_loss 11.14 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00270/00282]\tTime 0.56 (0.58)\tLoss 11.54 (nan)\n",
      "\t\tcls_loss 0.26 (0.29)\treg_loss 0.02 (0.02)\treco_loss 11.23 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [012][00280/00282]\tTime 0.60 (0.58)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.29)\treg_loss 0.01 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "[Train]: Epoch 12 finished with lr=0.00009550\n",
      "\n",
      "\n",
      "[Train]: Epoch 13 started\n",
      "Epoch: [013][00010/00282]\tTime 0.65 (0.65)\tLoss 11.00 (11.00)\n",
      "\t\tcls_loss 0.26 (0.26)\treg_loss 0.02 (0.02)\treco_loss 10.69 (10.69)\treco_cls_loss 0.02 (0.02)\n",
      "Epoch: [013][00020/00282]\tTime 0.57 (0.61)\tLoss 11.18 (11.09)\n",
      "\t\tcls_loss 0.28 (0.27)\treg_loss 0.01 (0.02)\treco_loss 10.88 (10.78)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [013][00030/00282]\tTime 0.56 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.31 (0.28)\treg_loss 0.01 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [013][00040/00282]\tTime 0.56 (0.58)\tLoss 12.49 (nan)\n",
      "\t\tcls_loss 0.29 (0.28)\treg_loss 0.03 (0.02)\treco_loss 12.14 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [013][00050/00282]\tTime 0.56 (0.58)\tLoss 13.35 (nan)\n",
      "\t\tcls_loss 0.30 (0.29)\treg_loss 0.03 (0.02)\treco_loss 13.00 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [013][00060/00282]\tTime 0.56 (0.57)\tLoss 12.26 (nan)\n",
      "\t\tcls_loss 0.28 (0.29)\treg_loss 0.03 (0.02)\treco_loss 11.92 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [013][00070/00282]\tTime 0.56 (0.57)\tLoss 10.67 (nan)\n",
      "\t\tcls_loss 0.28 (0.29)\treg_loss 0.03 (0.02)\treco_loss 10.34 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [013][00080/00282]\tTime 0.55 (0.57)\tLoss 11.39 (nan)\n",
      "\t\tcls_loss 0.19 (0.27)\treg_loss 0.01 (0.02)\treco_loss 11.17 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [013][00090/00282]\tTime 0.56 (0.57)\tLoss 12.46 (nan)\n",
      "\t\tcls_loss 0.26 (0.27)\treg_loss 0.01 (0.02)\treco_loss 12.17 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [013][00100/00282]\tTime 0.56 (0.57)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.31 (0.28)\treg_loss 0.01 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [013][00110/00282]\tTime 0.58 (0.57)\tLoss 11.23 (nan)\n",
      "\t\tcls_loss 0.28 (0.28)\treg_loss 0.02 (0.02)\treco_loss 10.92 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [013][00120/00282]\tTime 0.59 (0.57)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.28)\treg_loss 0.02 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.03 (0.00)\n",
      "Epoch: [013][00130/00282]\tTime 0.57 (0.57)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.30 (0.28)\treg_loss 0.01 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [013][00140/00282]\tTime 0.56 (0.57)\tLoss 10.42 (nan)\n",
      "\t\tcls_loss 0.29 (0.28)\treg_loss 0.01 (0.02)\treco_loss 10.10 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [013][00150/00282]\tTime 0.56 (0.57)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.29)\treg_loss 0.04 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.05 (0.01)\n",
      "Epoch: [013][00160/00282]\tTime 0.57 (0.57)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.29)\treg_loss 0.05 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [013][00170/00282]\tTime 0.57 (0.57)\tLoss 11.42 (nan)\n",
      "\t\tcls_loss 0.28 (0.29)\treg_loss 0.01 (0.02)\treco_loss 11.12 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [013][00180/00282]\tTime 0.56 (0.57)\tLoss 12.23 (nan)\n",
      "\t\tcls_loss 0.24 (0.29)\treg_loss 0.03 (0.02)\treco_loss 11.92 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [013][00190/00282]\tTime 0.57 (0.57)\tLoss 11.37 (nan)\n",
      "\t\tcls_loss 0.28 (0.29)\treg_loss 0.02 (0.02)\treco_loss 11.05 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [013][00200/00282]\tTime 0.57 (0.57)\tLoss 12.64 (nan)\n",
      "\t\tcls_loss 0.29 (0.29)\treg_loss 0.02 (0.02)\treco_loss 12.32 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [013][00210/00282]\tTime 0.57 (0.57)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.34 (0.29)\treg_loss 0.02 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.16 (0.01)\n",
      "Epoch: [013][00220/00282]\tTime 0.58 (0.57)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.31 (0.29)\treg_loss 0.02 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [013][00230/00282]\tTime 0.58 (0.57)\tLoss 10.92 (nan)\n",
      "\t\tcls_loss 0.28 (0.29)\treg_loss 0.01 (0.02)\treco_loss 10.63 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [013][00240/00282]\tTime 0.56 (0.57)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.29)\treg_loss 0.02 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.08 (0.01)\n",
      "Epoch: [013][00250/00282]\tTime 0.58 (0.57)\tLoss 10.25 (nan)\n",
      "\t\tcls_loss 0.26 (0.29)\treg_loss 0.01 (0.02)\treco_loss 9.97 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [013][00260/00282]\tTime 0.59 (0.57)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.33 (0.29)\treg_loss 0.01 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [013][00270/00282]\tTime 0.57 (0.57)\tLoss 12.28 (nan)\n",
      "\t\tcls_loss 0.29 (0.29)\treg_loss 0.02 (0.02)\treco_loss 11.95 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "Epoch: [013][00280/00282]\tTime 0.56 (0.57)\tLoss 11.79 (nan)\n",
      "\t\tcls_loss 0.20 (0.29)\treg_loss 0.01 (0.02)\treco_loss 11.57 (nan)\treco_cls_loss 0.00 (0.01)\n",
      "[Train]: Epoch 13 finished with lr=0.00002448\n",
      "\n",
      "\n",
      "[Train]: Epoch 14 started\n",
      "Epoch: [014][00010/00282]\tTime 0.67 (0.67)\tLoss 12.72 (12.72)\n",
      "\t\tcls_loss 0.30 (0.30)\treg_loss 0.03 (0.03)\treco_loss 12.38 (12.38)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00020/00282]\tTime 0.58 (0.62)\tLoss 10.96 (11.84)\n",
      "\t\tcls_loss 0.29 (0.30)\treg_loss 0.02 (0.02)\treco_loss 10.63 (11.50)\treco_cls_loss 0.01 (0.00)\n",
      "Epoch: [014][00030/00282]\tTime 0.57 (0.61)\tLoss 11.16 (11.61)\n",
      "\t\tcls_loss 0.23 (0.27)\treg_loss 0.01 (0.02)\treco_loss 10.91 (11.31)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00040/00282]\tTime 0.58 (0.60)\tLoss 10.83 (11.42)\n",
      "\t\tcls_loss 0.26 (0.27)\treg_loss 0.01 (0.02)\treco_loss 10.54 (11.11)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00050/00282]\tTime 0.59 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.29 (0.28)\treg_loss 0.01 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00060/00282]\tTime 0.60 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.28)\treg_loss 0.03 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00070/00282]\tTime 0.61 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.29)\treg_loss 0.03 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00080/00282]\tTime 0.58 (0.60)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.31 (0.29)\treg_loss 0.02 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00090/00282]\tTime 0.56 (0.59)\tLoss 11.67 (nan)\n",
      "\t\tcls_loss 0.25 (0.29)\treg_loss 0.03 (0.02)\treco_loss 11.36 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00100/00282]\tTime 0.57 (0.59)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.30 (0.29)\treg_loss 0.01 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00110/00282]\tTime 0.59 (0.59)\tLoss 11.73 (nan)\n",
      "\t\tcls_loss 0.26 (0.28)\treg_loss 0.02 (0.02)\treco_loss 11.44 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00120/00282]\tTime 0.56 (0.59)\tLoss 11.98 (nan)\n",
      "\t\tcls_loss 0.21 (0.28)\treg_loss 0.01 (0.02)\treco_loss 11.74 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00130/00282]\tTime 0.57 (0.59)\tLoss 11.68 (nan)\n",
      "\t\tcls_loss 0.25 (0.28)\treg_loss 0.02 (0.02)\treco_loss 11.39 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00140/00282]\tTime 0.57 (0.58)\tLoss 11.68 (nan)\n",
      "\t\tcls_loss 0.23 (0.27)\treg_loss 0.01 (0.02)\treco_loss 11.43 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00150/00282]\tTime 0.56 (0.58)\tLoss 11.13 (nan)\n",
      "\t\tcls_loss 0.23 (0.27)\treg_loss 0.03 (0.02)\treco_loss 10.83 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00160/00282]\tTime 0.56 (0.58)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.27)\treg_loss 0.01 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00170/00282]\tTime 0.56 (0.58)\tLoss 11.81 (nan)\n",
      "\t\tcls_loss 0.20 (0.27)\treg_loss 0.01 (0.02)\treco_loss 11.59 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00180/00282]\tTime 0.56 (0.58)\tLoss 11.84 (nan)\n",
      "\t\tcls_loss 0.29 (0.27)\treg_loss 0.01 (0.02)\treco_loss 11.53 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00190/00282]\tTime 0.55 (0.58)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.27)\treg_loss 0.03 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00200/00282]\tTime 0.55 (0.58)\tLoss 11.31 (nan)\n",
      "\t\tcls_loss 0.35 (0.28)\treg_loss 0.03 (0.02)\treco_loss 10.90 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00210/00282]\tTime 0.56 (0.58)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.32 (0.28)\treg_loss 0.02 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00220/00282]\tTime 0.60 (0.58)\tLoss 11.39 (nan)\n",
      "\t\tcls_loss 0.29 (0.28)\treg_loss 0.02 (0.02)\treco_loss 11.05 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00230/00282]\tTime 0.60 (0.58)\tLoss 11.41 (nan)\n",
      "\t\tcls_loss 0.28 (0.28)\treg_loss 0.01 (0.02)\treco_loss 11.11 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00240/00282]\tTime 0.57 (0.58)\tLoss 12.77 (nan)\n",
      "\t\tcls_loss 0.27 (0.28)\treg_loss 0.02 (0.02)\treco_loss 12.46 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00250/00282]\tTime 0.59 (0.58)\tLoss 11.85 (nan)\n",
      "\t\tcls_loss 0.19 (0.28)\treg_loss 0.01 (0.02)\treco_loss 11.64 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00260/00282]\tTime 0.61 (0.58)\tLoss nan (nan)\n",
      "\t\tcls_loss 0.29 (0.28)\treg_loss 0.02 (0.02)\treco_loss nan (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00270/00282]\tTime 0.60 (0.58)\tLoss 11.13 (nan)\n",
      "\t\tcls_loss 0.29 (0.28)\treg_loss 0.02 (0.02)\treco_loss 10.80 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "Epoch: [014][00280/00282]\tTime 0.58 (0.58)\tLoss 12.03 (nan)\n",
      "\t\tcls_loss 0.25 (0.28)\treg_loss 0.01 (0.02)\treco_loss 11.76 (nan)\treco_cls_loss 0.00 (0.00)\n",
      "[Train]: Epoch 14 finished with lr=0.00000001\n",
      "\n",
      "Test: [00010/00108]\tTime 0.18 (0.18)\n",
      "Test: [00020/00108]\tTime 0.12 (0.15)\n",
      "Test: [00030/00108]\tTime 0.12 (0.14)\n",
      "Test: [00040/00108]\tTime 0.12 (0.14)\n",
      "Test: [00050/00108]\tTime 0.13 (0.14)\n",
      "Test: [00060/00108]\tTime 0.13 (0.14)\n",
      "Test: [00070/00108]\tTime 0.12 (0.13)\n",
      "Test: [00080/00108]\tTime 0.13 (0.13)\n",
      "Test: [00090/00108]\tTime 0.12 (0.13)\n",
      "Test: [00100/00108]\tTime 0.13 (0.13)\n",
      "saving detection results...\n",
      "evaluion detection results...\n",
      "{'Fake': 0}\n",
      "[INIT] Loaded annotations from dev subset.\n",
      "\tNumber of ground truth instances: 94\n",
      "\tNumber of predictions: 10800\n",
      "\tFixed threshold for tiou score: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "[RESULTS] Performance on  detection task.\n",
      "Average-mAP: 0.9928590721464268\n",
      "Detection: average-mAP 99.286 mAP@0.50 99.921 mAP@0.55 99.921 mAP@0.60 99.921 mAP@0.65 99.921 mAP@0.70 99.921 mAP@0.75 99.921 mAP@0.80 98.880 mAP@0.85 98.880 mAP@0.90 98.880 mAP@0.95 96.693\n",
      "evaluion proposal results...\n",
      "{'Fake': 0}\n",
      "[INIT] Loaded annotations from dev subset.\n",
      "\tNumber of ground truth instances: 94\n",
      "\tNumber of proposals: 10800\n",
      "\tFixed threshold for tiou score: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "[RESULTS] Performance on  proposal task.\n",
      "\tArea Under the AR vs AN curve: 98.36808510638296%\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "train.run(config='configs/UMMAFormer/psynd_byola.yaml', eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'audio_feat_folder': './data/Psynd/feats/byola',\n",
      "             'audio_file_ext': None,\n",
      "             'audio_input_dim': 2048,\n",
      "             'crop_ratio': [0.9, 1.0],\n",
      "             'default_fps': None,\n",
      "             'downsample_rate': 1,\n",
      "             'feat_stride': 1,\n",
      "             'file_ext': '.npy',\n",
      "             'file_prefix': 'none',\n",
      "             'force_upsampling': True,\n",
      "             'input_dim': 0,\n",
      "             'json_file': './data/Psynd/annotations/metadatabyola.json',\n",
      "             'max_seq_len': 768,\n",
      "             'num_classes': 1,\n",
      "             'num_frames': 1,\n",
      "             'trunc_thresh': 0.5},\n",
      " 'dataset_name': 'psynd',\n",
      " 'devices': [0],\n",
      " 'init_rand_seed': 1234567891,\n",
      " 'loader': {'batch_size': 8, 'num_workers': 4},\n",
      " 'model': {'audio_input_dim': 2048,\n",
      "           'backbone_arch': (2, 2, 5),\n",
      "           'backbone_type': 'convHRLRFullResSelfAttTransformerRevised',\n",
      "           'embd_dim': 256,\n",
      "           'embd_kernel_size': 3,\n",
      "           'embd_with_ln': True,\n",
      "           'fpn_dim': 256,\n",
      "           'fpn_start_level': 0,\n",
      "           'fpn_type': 'fpn',\n",
      "           'fpn_with_ln': True,\n",
      "           'head_dim': 256,\n",
      "           'head_kernel_size': 3,\n",
      "           'head_num_layers': 3,\n",
      "           'head_with_ln': True,\n",
      "           'input_dim': 0,\n",
      "           'max_buffer_len_factor': 1.0,\n",
      "           'max_seq_len': 768,\n",
      "           'n_head': 4,\n",
      "           'n_mha_win_size': [7, 7, 7, 7, 7, -1],\n",
      "           'num_classes': 1,\n",
      "           'regression_range': [(0, 4),\n",
      "                                (4, 8),\n",
      "                                (8, 16),\n",
      "                                (16, 32),\n",
      "                                (32, 64),\n",
      "                                (64, 10000)],\n",
      "           'scale_factor': 2,\n",
      "           'test_cfg': {'duration_thresh': 0.001,\n",
      "                        'ext_score_file': None,\n",
      "                        'iou_threshold': 0.1,\n",
      "                        'max_seg_num': 100,\n",
      "                        'min_score': 0.001,\n",
      "                        'multiclass_nms': False,\n",
      "                        'nms_method': 'soft',\n",
      "                        'nms_sigma': 0.75,\n",
      "                        'pre_nms_thresh': 0.001,\n",
      "                        'pre_nms_topk': 2000,\n",
      "                        'voting_thresh': 0.9},\n",
      "           'train_cfg': {'center_sample': 'radius',\n",
      "                         'center_sample_radius': 1.5,\n",
      "                         'clip_grad_l2norm': 1.0,\n",
      "                         'cls_prior_prob': 0.01,\n",
      "                         'dropout': 0.0,\n",
      "                         'droppath': 0.1,\n",
      "                         'head_empty_cls': [],\n",
      "                         'init_loss_norm': 200,\n",
      "                         'label_smoothing': 0.1,\n",
      "                         'loss_weight': 2.0},\n",
      "           'use_abs_pe': True,\n",
      "           'use_rel_pe': False},\n",
      " 'model_name': 'AVLocPointTransformerRecoveryNoNorm',\n",
      " 'opt': {'epochs': 10,\n",
      "         'learning_rate': 0.001,\n",
      "         'momentum': 0.9,\n",
      "         'schedule_gamma': 0.1,\n",
      "         'schedule_steps': [],\n",
      "         'schedule_type': 'cosine',\n",
      "         'type': 'AdamW',\n",
      "         'warmup': True,\n",
      "         'warmup_epochs': 5,\n",
      "         'weight_decay': 0.05},\n",
      " 'output_folder': './paper_results/Revised',\n",
      " 'test_cfg': {'duration_thresh': 0.001,\n",
      "              'ext_score_file': None,\n",
      "              'iou_threshold': 0.1,\n",
      "              'max_seg_num': 100,\n",
      "              'min_score': 0.001,\n",
      "              'multiclass_nms': False,\n",
      "              'nms_method': 'soft',\n",
      "              'nms_sigma': 0.75,\n",
      "              'pre_nms_thresh': 0.001,\n",
      "              'pre_nms_topk': 2000,\n",
      "              'voting_thresh': 0.9},\n",
      " 'test_split': ['test'],\n",
      " 'train_cfg': {'center_sample': 'radius',\n",
      "               'center_sample_radius': 1.5,\n",
      "               'clip_grad_l2norm': 1.0,\n",
      "               'cls_prior_prob': 0.01,\n",
      "               'dropout': 0.0,\n",
      "               'droppath': 0.1,\n",
      "               'head_empty_cls': [],\n",
      "               'init_loss_norm': 200,\n",
      "               'label_smoothing': 0.1,\n",
      "               'loss_weight': 2.0},\n",
      " 'train_split': ['train'],\n",
      " 'val_split': ['dev']}\n",
      "['test'] subset has 79 videos\n",
      "=> loading checkpoint 'paper_results/Revised/psynd_byola_2025_05_22_15_25_13/epoch_015.pth.tar'\n",
      "Loading from EMA model ...\n",
      "\n",
      "Start testing model AVLocPointTransformerRecoveryNoNorm ...\n",
      "Test: [00010/00079]\tTime 0.18 (0.18)\n",
      "Test: [00020/00079]\tTime 0.14 (0.16)\n",
      "Test: [00030/00079]\tTime 0.13 (0.15)\n",
      "Test: [00040/00079]\tTime 0.13 (0.14)\n",
      "Test: [00050/00079]\tTime 0.13 (0.14)\n",
      "Test: [00060/00079]\tTime 0.14 (0.14)\n",
      "Test: [00070/00079]\tTime 0.13 (0.14)\n",
      "saving detection results...\n",
      "evaluion detection results...\n",
      "{'Fake': 0}\n",
      "[INIT] Loaded annotations from test subset.\n",
      "\tNumber of ground truth instances: 79\n",
      "\tNumber of predictions: 7900\n",
      "\tFixed threshold for tiou score: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "[RESULTS] Performance on  detection task.\n",
      "Average-mAP: 0.9741568020162259\n",
      "Detection: average-mAP 97.416 mAP@0.50 100.000 mAP@0.55 100.000 mAP@0.60 100.000 mAP@0.65 100.000 mAP@0.70 100.000 mAP@0.75 98.526 mAP@0.80 98.526 mAP@0.85 97.274 mAP@0.90 97.274 mAP@0.95 82.558\n",
      "evaluion proposal results...\n",
      "{'Fake': 0}\n",
      "[INIT] Loaded annotations from test subset.\n",
      "\tNumber of ground truth instances: 79\n",
      "\tNumber of proposals: 7900\n",
      "\tFixed threshold for tiou score: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "[RESULTS] Performance on  proposal task.\n",
      "\tArea Under the AR vs AN curve: 96.86962025316454%\n",
      "All done! Total time: 21.22 sec\n"
     ]
    }
   ],
   "source": [
    "eval.run(config='configs/UMMAFormer/psynd_byola.yaml', ckpt='paper_results/Revised/psynd_byola_2025_05_22_15_25_13/epoch_015.pth.tar', epoch=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Results:\n",
      "   average-mAP  mAP@0.50  mAP@0.55  mAP@0.60  mAP@0.65  mAP@0.70  mAP@0.75  \\\n",
      "0       97.416     100.0     100.0     100.0     100.0     100.0    98.526   \n",
      "\n",
      "   mAP@0.80  mAP@0.85  mAP@0.90  mAP@0.95  \n",
      "0    98.526    97.274    97.274    82.558  \n",
      "\n",
      "Proposal Results:\n",
      "    AR@10   AR@20   AR@50  AR@100\n",
      "0  97.848  97.848  97.848  97.848\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/ben/Thesis/UMMAFormerTest/paper_results/Revised/psynd_byola_2025_05_22_15_25_13/test_results.txt'\n",
    "detection_df, proposal_df = tsf.format_results_as_matrix(file_path)\n",
    "\n",
    "# Display the matrices\n",
    "print(\"Detection Results:\")\n",
    "print(detection_df)\n",
    "\n",
    "print(\"\\nProposal Results:\")\n",
    "print(proposal_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
